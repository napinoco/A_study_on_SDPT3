\documentclass{scrartcl}
\usepackage{graphicx,amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{appendix}
\newtheorem{example}{Example}
\newcommand{\inprod}[2]{\left\langle #1, \, #2 \right\rangle}

\title{A Study on the Algorithm and Implementation of SDPT3}
\author{
    Naoki Ito
    % \\Fast Retailing Co. Ltd.
    %\\ \texttt{naoki.b.ito@fastretailing.com}
}

\date{\today}

\begin{document}

\maketitle
\begin{abstract}
This technical report presents a comprehensive study of SDPT3, a widely used open-source MATLAB solver for semidefinite-quadratic-linear programming, which is based on the interior-point method.
It includes a self-contained and consistent description of the algorithm, with mathematical notation carefully aligned with the implementation. 
The aim is to offer a clear and structured reference for researchers and developers seeking to understand or build upon the implementation of SDPT3.
\end{abstract}

\tableofcontents
\section{Notations}\label{sec:notation}


For a positive integer $n$, let $\mathbb{R}^n$ denote the $n$-dimensional real vector space. In this paper, we treat elements $x$ of $\mathbb{R}^n$ as column vectors, with the $i$-th component denoted by $x_i$. The transpose of a vector $x$ is denoted by $x^T$.

For positive integers $m$ and $n$, let $\mathbb{R}^{m \times n}$ denote the space of $m \times n$ real matrices. The $(i,j)$ component of $x \in \mathbb{R}^{m \times n}$ is denoted by $x_{ij}$. Furthermore, the transpose of a matrix $x$ is denoted by $x^T$, and if $x \in \mathbb{R}^{n \times n}$ is invertible, its inverse is denoted by $x^{-1}$, and the transpose of the inverse by $x^{-T}$. The trace of a matrix $x$ is denoted by $\operatorname{trace}(x)$, and the determinant by $\operatorname{det}(x)$. The identity matrix is denoted by $I$. Additionally, we define the diagonal matrix $J$ as follows:
\[
    J = \begin{pmatrix}
         1 & 0 \ldots 0\\
         \substack{\displaystyle 0\\\displaystyle\vdots\\\displaystyle 0} & -I 
    \end{pmatrix},
\]
where $O$ denotes the zero matrix.

Regarding the notation $\|a\|$, if $a$ is a matrix, it represents the Frobenius norm, and if $a$ is a vector, it represents the L2 norm (Euclidean norm). Specifically,
\[
\|a\| = 
\begin{cases}
    \sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2} & \text{if } a \in \mathbb{R}^{m \times n},\\
    \sqrt{\sum_{i=1}^n a_i^2} & \text{if } a \in \mathbb{R}^n.
\end{cases}
\]

We define several sets as follows:
\begin{itemize}
    \item 
    \textbf{$n$-dimensional non-negative real cone:} \\
    \[
      \mathbb{R}^n_+ 
      = \{\,x \in \mathbb{R}^n \mid x \geq 0 \}.
    \]

    \item 
    \textbf{$n$-dimensional second-order cone:} \\
    \[
      \mathbb{Q}^n 
      = \{\, (x_0, \bar{x}^T)^T \in \mathbb{R}^n 
         \mid x_0 \in \mathbb{R}^1_+, \; \bar{x} \in \mathbb{R}^{n-1}, \; \|\bar{x}\| \le x_0 \}.
    \]\\
    Here, we define a non-negative real-valued function $\gamma : \mathbb{Q}^n \to \mathbb{R}^1_+$ on $\mathbb{Q}^n$ as
    \[
      \gamma(x) = \sqrt{x^T J\, x}.
    \]

    \item 
    \textbf{Space of $n$-dimensional real symmetric matrices:} \\
    \[
      \mathbb{S}^n = \{\, x \in \mathbb{R}^{n \times n} \mid x = x^T \}.
    \]

    \item 
    \textbf{$n$-dimensional real positive semi-definite cone:} \\
    \[
      \mathbb{S}^n_+ 
      = \{\, x \in \mathbb{S}^n \mid a^T x\, a \ge 0 \;\; \forall a \in \mathbb{R}^n \}.
    \]
\end{itemize}

The interior of a set $S$ is defined as
\[
  \operatorname{int}(S) 
  = \{\, x \in S \mid \exists\, \epsilon > 0 \quad \text{s.t.} \quad B(x,\; \epsilon) \subseteq S \},
\]
whereas the relative interior is defined as the interior within the affine hull of $S$:
\[
\operatorname{relint}(S)
=\{\,
x \in S 
\mid \exists \epsilon>0,\; \bigl(\operatorname{aff}(S)\cap B(x,\epsilon)\bigr)\,\subseteq\,S
\},
\]
where $\operatorname{aff}(S)$ denotes the affine hull of $S$, and $B(x,\epsilon)$ denotes the open ball of radius $\epsilon$ centered at $x$.


\section{Problem Definition}

In this paper, we consider the following primal problem \((P)\) and its corresponding dual problem \((D)\):
\begin{equation*}
    \everymath{\displaystyle}
    \renewcommand{\arraystretch}{2.0}
    (P)~
    \left|
    \begin{array}{cl}
         \min_{x} & \displaystyle 
             \sum_{p \in P} \Bigl(\inprod{c^p}{x^p}_p\Bigr) 
             \;+\;\sum_{p \in P} \phi^p\bigl(x^p;\,\nu^p\bigr) \\[3pt]
         \text{s.t.} 
         & \displaystyle 
             \sum_{p \in P} \inprod{a^p_{k}}{x^p}_p 
             \;=\; b_k \quad (k= 1,2,\ldots,m), \\[3pt]
         & x^p \;\in\; \mathbb{K}^p \quad (p\in P),
    \end{array}
    \right.
    \qquad
    (D)~
    \left|
    \begin{array}{cl}
         \max_{y,z} & \displaystyle 
            \sum_{k=1}^m b_k\, y_k 
            \;+\;\sum_{p \in P} \phi^{p*}\bigl(z^p;\,\nu^p\bigr) \\[3pt]
         \text{s.t.} 
         & \displaystyle 
            c^p \;-\; \sum_{k=1}^m a^p_k\,y_k \;=\; z^p \quad (p\in P), \\[3pt]
         & z^p \;\in\; (\mathbb{K}^p)^* \quad (p\in P).
    \end{array}
    \right.
\end{equation*}

Here, the following parameters are assumed to be given:
\begin{itemize}
    \item A positive integer $p_{\max}$ representing the number of cone blocks, and the index set $P = \{1, 2, \ldots, p_{\max}\}$.
    \item For each block $p \in P$, a positive integer $n_p$ representing the dimension of the variables, and a positive integer $m$ representing the number of constraints.
    \item The cone $\mathbb{K}^p$ for each $p \in P$, which is one of $\mathbb{S}^{n_p}_+$, $\mathbb{Q}^{n_p}$, $\mathbb{R}^{n_p}_+$, or $\mathbb{R}^{n_p}$.
    \item Coefficients $c^p$ and $a^p_k$ for $k = 1, \ldots, m$ and $p \in P$, where $c^p, a^p_k \in \mathbb{S}^{n_p}$ if $\mathbb{K}^p = \mathbb{S}^{n_p}_+$, and $\in \mathbb{R}^{n_p}$ otherwise.
    \item A coefficient vector $b \in \mathbb{R}^m$.
    \item Non-negative parameters $\nu^p \ge 0$ for all $p \in P$.
\end{itemize}

The dual cone $(\mathbb{K}^p)^*$ is defined as:
\[
(\mathbb{K}^p)^* = 
\begin{cases}
    \mathbb{S}_{+}^{n_p} & \text{if } \mathbb{K}^{p} = \mathbb{S}_{+}^{n_p},\\
    \mathbb{Q}^{n_p}     & \text{if } \mathbb{K}^{p} = \mathbb{Q}^{n_p},\\
    \mathbb{R}_{+}^{n_p} & \text{if } \mathbb{K}^{p} = \mathbb{R}_{+}^{n_p},\\
    \{0\}                & \text{if } \mathbb{K}^{p} = \mathbb{R}^{n_p}.
\end{cases}
\]
The inner product $\left\langle a, x \right\rangle_p$ is defined as:
\[
\left\langle a, x \right\rangle_p = 
\begin{cases}
    \operatorname{trace}(a^T x) 
    = \sum_{i=1}^{n_p} \sum_{j=1}^{n_p} a_{ij} x_{ij}, 
    & \text{if } \mathbb{K}^p = \mathbb{S}_{+}^{n_p},\\[6pt]
    a^T x = \sum_{i=1}^{n_p} a_i x_i, 
    & \text{otherwise}.
\end{cases}
\]

The function $\phi^p : \mathbb{K}^p \to \mathbb{R}_+$ is a barrier function defined as:
\[
\phi^p(x^p;\, \nu^p) =
\begin{cases}
    -\nu^p \log \det(x^p), & \text{if } \mathbb{K}^{p} = \mathbb{S}_{+}^{n_p},\\
    -\nu^p \log \gamma(x^p), & \text{if } \mathbb{K}^{p} = \mathbb{Q}^{n_p},\\
    -\sum_{i=1}^{n_p} \nu^p \log x^p_i, & \text{if } \mathbb{K}^{p} = \mathbb{R}_{+}^{n_p},\\
    0, & \text{if } \mathbb{K}^{p} = \mathbb{R}^{n_p},
\end{cases}
\]
where, as introduced in Section~\ref{sec:notation}, $\gamma(x^p) = \sqrt{(x^p)^T J x^p}$.

The function $\phi^{p*} : \mathbb{K}^p \to \mathbb{R}_+$ is the convex conjugate of $\phi^p$, defined by:
\[
\phi^{p*}(z^p;\, \nu^p) =
\begin{cases}
    \nu^p \log \det(z^p) + n_p \nu^p (1 - \log \nu^p), 
    & \text{if } \mathbb{K}^{p} = \mathbb{S}_{+}^{n_p},\\[4pt]
    \nu^p \log \gamma(z^p) + \nu^p (1 - \log \nu^p), 
    & \text{if } \mathbb{K}^{p} = \mathbb{Q}^{n_p},\\[4pt]
    \sum_{i=1}^{n_p} \left( \nu^p \log z^p_i + \nu^p (1 - \log \nu^p) \right), 
    & \text{if } \mathbb{K}^{p} = \mathbb{R}_{+}^{n_p},\\[3pt]
    0, & \text{if } \mathbb{K}^{p} = \mathbb{R}^{n_p}.
\end{cases}
\]

When $\nu^p = 0$ for all $p \in P$, problems $(P)$ and $(D)$ reduce to the standard form of conic linear programming. By considering the case $\nu^p > 0$, we can handle a broader class of problems, including those involving log-determinant terms.



\section{Infeasible Primal-Dual Path-Following Interior-Point Method}
\label{sec:infeasible_IPM}

In this chapter, we discuss an interior-point method designed to converge to a feasible solution through iterations, even if the initial point is infeasible, within the framework of the primal-dual path-following method. In practical applications, there may be errors in the problem data or the feasible solution may be unknown, making such an "infeasible sequence" version of the algorithm highly significant.

\medskip

\noindent
For simplicity of notation, we define two linear mappings 
$\mathcal{A}^p : \mathbb{K}^p \to \mathbb{R}^m$ and 
$(\mathcal{A}^p)^T : \mathbb{R}^m \to \mathbb{K}^p$ 
as follows:
\[
  \mathcal{A}^p x^p
  := 
  \begin{pmatrix}
      \left\langle a^p_1, x^p \right\rangle_p \\
      \left\langle a^p_2, x^p \right\rangle_p \\
      \vdots \\
      \left\langle a^p_m, x^p \right\rangle_p
  \end{pmatrix},
  \qquad
  (\mathcal{A}^p)^T y
  :=
  \sum_{k=1}^m a^p_k y_k.
\]
Then, the primal problem $(P)$ and the dual problem $(D)$ can be written as follows:
\[
  (P)\;:\;
  \left\{
  \begin{aligned}
      &\min_{x}
       && \sum_{p \in P} \left( \left\langle c^p, x^p \right\rangle_p - \phi^p(x^p;\, \nu^p) \right) \\
      &\text{s.t.}
       && \sum_{p \in P} \mathcal{A}^p x^p = b,\\
      & && x^p \in \mathbb{K}^p \quad (\forall p \in P),
  \end{aligned}
  \right.
  \quad
  (D)\;:\;
  \left\{
  \begin{aligned}
      &\max_{y,z}
       && \sum_{k=1}^m b_k y_k 
          + \sum_{p \in P} \phi^{p*}(z^p;\, \nu^p) \\
      &\text{s.t.}
       && c^p - (\mathcal{A}^p)^T y = z^p \quad (\forall p \in P),\\
      & && z^p \in (\mathbb{K}^p)^* \quad (\forall p \in P).
  \end{aligned}
  \right.
\]
Here, we define $P^u := \{\, p \in P \mid \mathbb{K}^p = \mathbb{R}^{n_p} \}\,$\footnote{%
  The superscript $u$ stands for "unrestricted," meaning $\mathbb{K}^p = \mathbb{R}^{n_p}$.
}.

\medskip

\noindent
Consider the following KKT conditions, which are necessary for the optimality of the primal-dual problem:
\begin{equation}
    \everymath{\displaystyle}
    \renewcommand{\arraystretch}{2.5}
    \left\{
    \begin{array}{ll}
        \sum_{p \in P} \mathcal{A}^p x^p - b = 0, & \\[-4pt]
        (\mathcal{A}^p)^T y + z^p - c^p = 0, & \quad (p \in P),\\[-4pt]
        x^p \circ z^p - \nu^p e^p = 0, & \quad (p \in P \setminus P^u),\\[-4pt]
        x^p \in \mathbb{K}^p,\; y \in \mathbb{R}^m,\; z^p \in (\mathbb{K}^p)^*, & \quad (p \in P).
    \end{array}
    \right.
    \label{eq:KKTcond}
\end{equation}
Here, the bilinear mapping $x^p \circ z^p$ is defined for each block $p \in P \setminus P^u$ as follows:
\[
  x^p \circ z^p = 
  \begin{cases}
    \frac{1}{2} \left( x^p (z^p)^T + (z^p)^T x^p \right), 
      & \text{if } \mathbb{K}^p = \mathbb{S}^{n_p}_+,\\[4pt]
    \left( (x^p)^T z^p;\; x^p_0 \bar{z}^p + z^p_0 \bar{x}^p \right),
      & \text{if } \mathbb{K}^p = \mathbb{Q}^{n_p},\\[4pt]
    \operatorname{diag}(x^p) z^p,
      & \text{if } \mathbb{K}^p = \mathbb{R}^{n_p}_+ %\text{ or }\mathbb{R}^{n_p}.
  \end{cases}
\]
This operation is called the Jordan product on $\mathbb{K}^p$. In the case of $\mathbb{S}^{n_p}_+$, it is usually written as $x^p \circ z^p = \frac{1}{2}(x^p z^p + z^p x^p)$, but in this paper, we extend this operator to non-symmetric matrices and use the notation $x^p \circ z^p = \frac{1}{2} \left( x^p (z^p)^T + z^p (x^p)^T \right)$. Note that the extension of the operator $\circ$ to non-symmetric matrices does not satisfy the definition of Jordan algebra \cite{Faraut1994}, but it allows for unified and simplified notation (especially in equations like \eqref{eq:NewtonKKT}). Furthermore, $e^p$ is the identity element for the operator $\circ$, given by:
\[
  e^p = 
  \begin{cases}
    I, & \mathbb{K}^p = \mathbb{S}^{n_p}_+,\\[3pt]
    (1,\,0,\ldots,0)^T, & \mathbb{K}^p = \mathbb{Q}^{n_p},\\[3pt]
    (1,\,1,\ldots,1)^T, & \mathbb{K}^p = \mathbb{R}^{n_p}_+ %\text{ or }\mathbb{R}^{n_p}.
  \end{cases}
\]

\medskip

Now, consider replacing $\nu^p$ ($p \in P$) with a positive constant $\mu > 0$. Then, the KKT conditions \eqref{eq:KKTcond} are known to provide a unique solution $\left( x(\mu),\, y(\mu),\, z(\mu) \right)$ within the feasible region. This solution changes smoothly with the value of $\mu$, forming a central path $T$:
\[
  T = \left\{ \left( x(\mu),\, y(\mu),\, z(\mu) \right) \mid \mu > 0 \right\}.
\]
In the actual path-following method, the properties of this path are utilized to simultaneously solve both the primal and dual problems.

\medskip

In the following sections, we will detail the main components of this algorithm, including the definition and computation of the search direction, the choice of step size, and convergence criteria.



\bigskip
\subsection{Search Directions} \label{sec:direction}

\subsubsection{Framework of the Search Direction}

Here, we assume that a tentative solution $(x,\,y,\,z) \in \operatorname{int}(\mathbb{K}) \times \mathbb{R}^m \times \operatorname{int}(\mathbb{K})$ is given.
Let $\mathbb{K} := \mathbb{K}^1 \times \cdots \times \mathbb{K}^{p_{\max}}$, and denote $x = (x^1, \ldots, x^{p_{\max}})$ and $z = (z^1, \ldots, z^{p_{\max}})$.

Using the parameter $\sigma \in [0,1)$ and the scaling matrices $G^p$ for each block $p \in P$, the search direction $(\Delta x,\, \Delta y,\, \Delta z)$ is given as the solution to the following equations:
\begin{equation}
    \renewcommand{\arraystretch}{2.5}
    \left\{
    \begin{array}{rll}
         \sum_{p \in P} \mathcal{A}^p \Delta x^p & = R_{prim} := b - \sum_{p \in P} \mathcal{A}^p x^p &  \\
         (\mathcal{A}^p)^T \Delta y + \Delta z^p & = R_{dual}^p := c^p - z^p - (\mathcal{A}^p)^T y & (p \in P) \\
         \mathcal{E}^p \Delta x^p + \mathcal{F}^p \Delta z^p & = R_{comp}^p := \max\{\sigma \mu, \nu^p\} e^p - (G^p x^p) \circ ((G^p)^{-1} z^p) & (p \in P \setminus P^u)
    \end{array}
    \right.
    \label{eq:NewtonKKT}
\end{equation}
where
\begin{equation}
  \mu := \frac{\sum_{p \in P \mid_{\nu^p=0}} \left\langle x^p, z^p \right\rangle}
              {\sum_{p \in P \mid_{\nu^p=0}} n^p}
  \label{eq:mu}
\end{equation}
is defined.
Moreover, $\mathcal{E}^p : \mathbb{K}^p \to \mathbb{K}^p$ and $\mathcal{F}^p : \mathbb{K}^p \to \mathbb{K}^p$ are linear mappings defined as
\[
  \mathcal{E}^p \Delta x^p
    := (G^p \Delta x^p) \circ ((G^p)^{-1} z^p),
  \quad
  \mathcal{F}^p \Delta z^p
    := (G^p x^p) \circ ((G^p)^{-1} \Delta z^p).
\]

Equation \eqref{eq:NewtonKKT} can be interpreted as the Newton step to approximately satisfy the primal feasibility, dual feasibility, and complementarity conditions of the KKT conditions \eqref{eq:KKTcond}.
By introducing the scaling matrices $G^p$ according to the cone structure of each block, the complementarity condition is appropriately transformed and symmetrized.
The parameter $\sigma$ is used to track the central path $T$, and $\sigma \mu$ serves as the "target duality gap."

The choice of scaling matrices $G^p$ affects the linearization of the complementarity part, impacting numerical stability and efficiency in sparse matrix processing.
Depending on the application field and problem characteristics, the appropriate search direction should be chosen.

\medskip


\subsubsection{AHO Direction}

First, consider the simplest scaling case where $G^p = I$. This is known as the Alizadeh-Haeberly-Overton (AHO) search direction \cite{Alizadeh1998} in the context of semidefinite programming and second-order cone programming. The AHO search direction is numerically stable but has a high computational cost per iteration. For simple cones such as $\mathbb{K}^p = \mathbb{R}^{n_p}_+$ and $\mathbb{K}^p = \mathbb{R}^{n_p}$, it is customary to use $G^p = I$.

\medskip

\subsubsection{HKM Direction}

\noindent

For $\mathbb{K}^p = \mathbb{S}^{n_p}_+$ or $\mathbb{K}^p = \mathbb{Q}^{n_p}$, consider the following scaling matrices $G^p$.

\paragraph{Case 1: \(\mathbb{K}^p = \mathbb{S}^{n_p}_+\).}
\[
  G^p := (z^p)^{\tfrac12}.
\]
Since $z^p$ is a positive semidefinite matrix, $(z^p)^{1/2}$ is uniquely defined. In this case, $G^p e^p (G^p)^T = z^p$ holds.

\paragraph{Case 2: \(\mathbb{K}^p = \mathbb{Q}^{n_p}\).}
\[
  \omega^p := \gamma(z^p), 
  \quad
  t^p := \frac{1}{\gamma(z^p)}\, z^p,
  \quad
  G^p :=
  \omega^p
  \begin{pmatrix}
    t^p_0 & (\bar{t}^p)^T \\
    \bar{t}^p & I + \frac{1}{1+t^p_0}\,\bar{t}^p(\bar{t}^p)^T
  \end{pmatrix}.
\]
Here, $\gamma(\cdot)$ is the norm function for the second-order cone (see Chapter 2). In this case, $G^p e^p = z^p$ holds.

\medskip

The search direction obtained by solving equation \eqref{eq:NewtonKKT} with these scaling matrices $G^p$ is called the HKM search direction. The HKM search direction is also known as the HRVW/KSH/M direction (Helmberg--Rendl--Vanderbei--Wolkowicz / Kojima--Shindoh--Hara / Monteiro direction) \cite{Kojima1997,Monteiro1997}. The HKM search direction is known for leveraging the sparsity of the problem and being numerically stable.

\subsubsection{NT Direction}

For $\mathbb{K}^p = \mathbb{S}^{n_p}_+$ or $\mathbb{K}^p = \mathbb{Q}^{n_p}$, consider the following scaling matrices $G^p$.

\paragraph{Case 1: \(\mathbb{K}^p = \mathbb{S}^{n_p}_+\).}
\[
  G^p 
  := 
    \Bigl( (x^p)^{\frac12} \bigl( (x^p)^{\frac12} z^p (x^p)^{\frac12} \bigr)^{-\frac12} (x^p)^{\frac12} \Bigr)^{-\frac12}.
\]
In this case, $(G^p)^{-1} z^p (G^p)^{-T} = G^p x^p (G^p)^T$ holds. In practice, $W^p := (G^p)^2$ is efficiently computed using eigenvalue decomposition or Cholesky decomposition.

\paragraph{Case 2: \(\mathbb{K}^p = \mathbb{Q}^{n_p}\).}
\begin{equation}
    \omega^p := \sqrt{\frac{\gamma(z^p)}{\gamma(x^p)}}, 
    \quad 
    \xi^p 
    := \begin{pmatrix} \xi^p_0 \\ \bar{\xi}^p \end{pmatrix} 
    = \begin{pmatrix}
        \frac{1}{\omega^p} z^p_0 + \omega^p x^p_0 \\
        \frac{1}{\omega^p} \bar{z}^p - \omega^p \bar{x}^p \\
    \end{pmatrix},
    \quad
    t^p := \frac{1}{\gamma(\xi^p)}\xi^p
    \label{eq:scaling_mat_NT_socp_aux}
\end{equation}
\begin{equation}
    G^p := \omega^p \begin{pmatrix}
        t^p_0 & (\bar{t}^p)^T \\
        \bar{t}^p & I+\frac{1}{1 + t^p_0} \bar{t}^p(\bar{t}^p)^T
    \end{pmatrix}
    \label{eq:scaling_mat_NT_socp}
\end{equation}
In this case, $(G^p)^{-1} z^p = G^p x^p$ holds.

\medskip

The search direction obtained by solving equation \eqref{eq:NewtonKKT} with these scaling matrices $G^p$ is called the Nesterov--Todd (NT) search direction \cite{Nesterov1997,todd1998}. The NT search direction is based on the theory of self-concordant barriers and is known for its strong theoretical guarantees of global convergence.

\medskip

In SDPT3, the options \texttt{HKM} and \texttt{NT} can be selected for search directions for $\mathbb{K}^p = \mathbb{S}^{n_p}_+$ and $\mathbb{K}^p = \mathbb{Q}^{n_p}$, but regardless of the option chosen, $G^p = I$ is used for $\mathbb{K}^p = \mathbb{R}^{n_p}_+$ and $\mathbb{K}^p = \mathbb{R}^{n_p}$.
\medskip



\subsection{Reduction of the equation}
In this section, we describe the procedure to \textbf{reduce} the search direction equation \eqref{eq:NewtonKKT} defined in the previous section into a more manageable form. Specifically, we eliminate $\Delta x^p$ and $\Delta z^p$ to obtain an equation in terms of $\Delta y$, resulting in the so-called Schur complement system.

First, by solving the following two equations from \eqref{eq:NewtonKKT} for $\Delta z^p$ and $\Delta x^p$ respectively, we obtain the equation in terms of $\Delta y$:
\[
  (\mathcal{A}^p)^T \Delta y + \Delta z^p = R_{dual}^p,
  \quad
  \mathcal{E}^p \Delta x^p + \mathcal{F}^p \Delta z^p = R_{comp}^p
\]
\[
    \everymath{\displaystyle}
    \renewcommand{\arraystretch}{2.5}
    \left\{
    \begin{array}{rll}
    \Delta z^p &= R_{dual}^p - \mathcal{A}^p(\Delta y)  & (p\in P)\\
    \Delta x^p &= (\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p(\Delta z^p) \\
               &= (\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p(R_{dual}^p - \mathcal{A}^p(\Delta y))  & (p\in P \text{ s.t. } \mathbb{K}^p \neq \mathbb{R})
   \end{array}
   \right.
   \label{eq:sol_x_z}
\]
where $\mathcal{H}^p := (\mathcal{E}^p)^{-1}\mathcal{F}^p$. The existence of $(\mathcal{E}^p)^{-1}$ is non-trivial, but it is known to exist if $G^p$ is positive definite \cite{todd1998}. Since all $G^p$ introduced in Section~\ref{sec:direction} are positive definite, $(\mathcal{E}^p)^{-1}$ exists. The specific calculation methods for $(\mathcal{E}^p)^{-1}$ and $\mathcal{H}^p$ will be discussed later.

Substituting \eqref{eq:sol_x_z} into the first equation of \eqref{eq:NewtonKKT}, we obtain:
\[
    \left\{
    \begin{aligned}
        \sum_{p \in P\setminus P^u} \mathcal{A}^p\mathcal{H}^p(\mathcal{A}^p)^T\Delta y + \sum_{p \in P^u} \mathcal{A}^p(\Delta x^p) 
            &= R_{prim} - \sum_{p \in P\setminus P^u} \mathcal{A}^p((\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p R_{dual}^p) \\
        (\mathcal{A}^p)^T \Delta y 
            &= R^p_{dual} \qquad (p\in P^u)
    \end{aligned}
    \right.
    \label{eq:Schur_complement}
\]

Next, we consider reducing these to matrix representations. Let the matrix $M^p \in \mathbb{S}^{m}$ satisfy
\[
    \mathcal{A}^p\mathcal{H}^p(\mathcal{A}^p)^T \Delta y= M^p \Delta y,
\]
and define:
\begin{align*}
    M &= \sum_{p \in P \setminus P^u} M^p \\
    h &= R_{prim} - \sum_{p \in P \setminus P^u} \mathcal{A}^p\big((\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p R_{dual}^p\big)\\
    A^p &= \begin{pmatrix}
        (a^p_1)^T\\
        (a^p_2)^T\\
        \vdots\\
        (a^p_m)^T
    \end{pmatrix} \in \mathbb{R}^{m\times n_p} \quad (p\in P)\\
    A^u &= A^p ~ (p\in P^u)\text{ concatenated horizontally}\\
    R^u_{dual} &= R^p_{dual} ~ (p\in P^u)\text{ concatenated vertically}\\
    \Delta x^u &= \Delta x^p ~ (p\in P^u)\text{ concatenated vertically}
\end{align*}
Then, the equation \eqref{eq:Schur_complement} can be represented in matrix form as:
\[
    \underbrace{\left(\begin{array}{cc}
        M   & A^u \\
        A^u & O
    \end{array}\right)}_{\mathcal{M}}
    \left(\begin{array}{c}
        \Delta y   \\
        \Delta x^u 
    \end{array}\right) 
    = 
    \left(\begin{array}{c}
         h  \\
         R_{dual}^u 
    \end{array}
    \right)
    \label{eq:Schur_complement_Mat}
\]

In the following, we describe the specific expressions for $(\mathcal{E}^p)^{-1}R^p_{comp}$, $\mathcal{H}^p R^p_{dual}$, and the matrix $M^p$ for HKM and NT search directions. For both HKM and NT search directions, we have:
\[
    (\mathcal{E}^p)^{-1}R^p_{comp} = \max\{\sigma\mu, \nu^p\}(z^p)^{-J} - x^p \qquad (p \in P)
\]
where $(z^p)^{-J}$ is the inverse element of $z^p$ with respect to the Jordan product $\circ$:
\[
    (z^p)^{-J} = \begin{cases}
        (z^p)^{-1} & \text{if} ~ \mathbb{K}^p=\mathbb{S}^{n_p}_+ \\
        \frac{1}{(\gamma(z^p))^2} J z^p & \text{if} ~ \mathbb{K}^p=\mathbb{Q}^{n_p} \\
        (1/z_1, \ldots, 1/z_{n_p})^T & \text{if} ~ \mathbb{K}^p=\mathbb{R}^{n_p}_+.
    \end{cases}
\]

When the search direction option is set to \texttt{HKM}, we have:
\begin{align}
    \mathcal{H}^p R^p_{dual} &= \begin{cases}
        \frac{1}{2}(x^p R^p_{dual} (z^p)^{-1} + (z^p)^{-1} R^p_{dual} x^p) & \text{if} ~ \mathbb{K}^p=\mathbb{S}^{n_p}_+\\
        - \Big( (x^p)^T J (z^p)^{-J} \Big) J R^p_{dual} + \inprod{(z^p)^{-J}}{R^p_{dual}} x^p + \inprod{R^p_{dual}}{x^p} (z^p)^{-J} & \text{if} ~ \mathbb{K}^p=\mathbb{Q}^{n_p}\\
        \operatorname{diag}(x^p) \operatorname{diag}(z^p)^{-1} \Delta z^p & \text{if} ~ \mathbb{K}^p=\mathbb{R}^{n_p}_+
    \end{cases} \label{eq:HKM_HRd}\\
    M^p &= \begin{cases} 
        \text{a matrix whose $(k,\ell)$-elements are given by } \inprod{a^p_k}{x^p a^p_\ell (z^p)^{-1}} & \text{if} ~ \mathbb{K}^p=\mathbb{S}^{n_p}_+ \\
        -\Big( (x^p)^T J (z^p)^{-1} \Big) A^p J (A^p)^T + (A^p x^p)(A^p (z^p)^{-1})^T + (A^p (z^p)^{-1})(A^p x^p)^T& \text{if} ~ \mathbb{K}^p=\mathbb{Q}^{n_p} \\
        A^p \operatorname{diag}(x^p) \operatorname{diag}(z^p)^{-1} (A^p)^T & \text{if} ~ \mathbb{K}^p=\mathbb{R}^{n_p}_+
    \end{cases}
\end{align}

When the search direction option is set to NT, we have:
\begin{align}
    \mathcal{H}^p R^p_{dual} &= \begin{cases}
        W^p R^p_{dual} W^p & \text{if} ~ \mathbb{K}^p=\mathbb{S}^{n_p}_+ \\
        \frac{1}{(\omega^p)^2} \Big(-J R^p_{dual} + 2\inprod{R^p_{dual}}{t^p}t^p\Big) & \text{if} ~ \mathbb{K}^p=\mathbb{Q}^{n_p}\\
        \operatorname{diag}(x^p) \operatorname{diag}(z^p)^{-1} R^p_{dual} & \text{if} ~ \mathbb{K}^p=\mathbb{R}^{n_p}_+
    \end{cases}\\
    M^p &= \begin{cases}
        \text{a matrix whose $(k,\ell)$-elements are given by } \inprod{a^p_k}{W^p a^p_\ell W^p} & \text{if} ~ \mathbb{K}^p=\mathbb{S}^{n_p}_+\\
        \frac{1}{(\omega^p)^2}\Big(-A^p J (A^p)^T + 2 (A^p t^p)(A^p t^p)^T \Big) & \text{if} ~ \mathbb{K}^p=\mathbb{Q}^{n_p} \\
        A^p \operatorname{diag}(x^p) \operatorname{diag}(z^p)^{-1} (A^p)^T & \text{if} ~ \mathbb{K}^p=\mathbb{R}^{n_p}_+
    \end{cases} \label{eq:NT_M}
\end{align}
where $W^p = (G^p)^{-2} = (x^p)^{-\frac{1}{2}}((x^p)^{\frac{1}{2}} z^p (x^p)^{\frac{1}{2}})^{\frac{1}{2}} (x^p)^{-\frac{1}{2}}$, and $\omega^p$, $t^p$ are defined as in \eqref{eq:scaling_mat_NT_socp_aux}.

Although $W^p$ appears to be very complex, it can be calculated using the following procedure \cite{todd1998}:
\begin{enumerate}
    \item First, perform the Cholesky decomposition of $z^p$ to obtain the upper triangular matrix $U$, i.e., $z^p=U^TU$.
    \item Next, perform the eigenvalue decomposition of $U x^p U$ to obtain the orthogonal matrix $V$ and the diagonal matrix $\Lambda$ with eigenvalues on the diagonal, i.e., $U x^p U = V \Lambda V^T$.
    \item Now, let $S=\Lambda^\frac{1}{4}(U^{-1}V)^T$, then $W^p=S^T S$.
\end{enumerate}

A brief guide on the derivation of \eqref{eq:HKM_HRd}--\eqref{eq:NT_M} can be found in Appendix~\ref{sec:guide_for_dir_eq}.

\subsection{Solving the reduced equation}
In many cases, the matrix $\mathcal{M}$ is very ill-conditioned, making direct solution of \eqref{eq:Schur_complement_Mat} numerically unstable. Therefore, iterative refinement methods are recommended. In practice, SDPT3 \cite{toh1999} uses the Symmetric Quasi-Minimal Residual (SQMR) method \cite{Freund1994} with $\mathcal{M}^{-1}$ as the preconditioning matrix to solve \eqref{eq:Schur_complement_Mat}.

In the SQMR method, it is necessary to repeatedly calculate the product of the matrix $\mathcal{M}^{-1}$ and a vector for preconditioning. Two approaches can be considered for efficient and accurate calculation:
The first approach is to perform LU decomposition of the entire matrix $\mathcal{M}$ in advance. This allows the product of $\mathcal{M}^{-1}$ and a vector to be calculated by forward-backward substitution.
% (This method is superior in terms of accuracy because it does not explicitly calculate the inverse of the ill-conditioned $\mathcal{M}$.)

If $M$ is positive definite, another approach can be taken. Using the Schur complement $S := (A^u)^T M^{-1} A^u - O$, we can write:
\[
\mathcal{M}^{-1}=\begin{pmatrix}
    M^{-1} + M^{-1} A^u S^{-1} (A^u)^T M^{-1} & -M^{-1} A^u S^{-1} \\
    -S^{-1} (A^u)^T M^{-1} & S^{-1}
\end{pmatrix}
\]
Thus, we obtain:
\[
    \mathcal{M}^{-1}\begin{pmatrix}u \\ v \end{pmatrix} = \begin{pmatrix} \hat{u} - M^{-1} A^u \hat{v} \\ \hat{v} \end{pmatrix}
\]
where $\hat{u} = M^{-1} u$ and $\hat{v} = S^{-1}\big((A^u)^T \hat{u} - v \big)$.
Therefore, if the Cholesky decomposition of $M$ (more efficient than LU decomposition) and the LU decomposition of $S$ are calculated in advance, the calculation of the product of $\mathcal{M}^{-1}$ and a vector reduces to three forward-backward substitutions for $M$ and two forward-backward substitutions for $S$.
Although the method using Cholesky decomposition is efficient, in practice, due to the presence of dependent constraints and numerical errors, $M$ may not be positive definite, and $S$ may also be very ill-conditioned.

In the implementation of SDPT3, the second method is tried first, and if the Cholesky decomposition of $M$ fails or if $S=LU$ is found to be extremely ill-conditioned (specifically, if the ratio of the maximum to minimum diagonal elements of $U$ is greater than $10^{x}$), the first method of LU decomposition of the entire $\mathcal{M}$ is adopted.

Both methods can be further optimized by exploiting the sparsity of the problem. Details are discussed in Section~\ref{sec:exploit_sparsity_socp_lp}.




\subsection{Step size} \label{sec:step_size}
In this section, we explain the method for calculating the step sizes $\alpha_P$, $\alpha_D \in [0, 1]$ used to determine the next iteration point 
\[
  (x^+,\,y^+,\,z^+) 
  \;=\; 
  \bigl(x + \alpha_P\,\Delta x,\;\; y + \alpha_D\,\Delta y,\;\; z + \alpha_D\,\Delta z\bigr).
\]

The iteration points must satisfy
\[
  x + \alpha_P \Delta x \;\in\; \operatorname{int}\bigl(\mathbb{K}\bigr),
  \quad
  z + \alpha_D \Delta z \;\in\; \operatorname{relint}\bigl((\mathbb{K})^*\bigr),
\]
thus we calculate
\[
  \alpha_x  := \sup \bigl\{\,\alpha \ge 0 \mid x + \alpha \,\Delta x \in \mathbb{K}\bigr\},
  \quad
  \alpha_z  := \sup \bigl\{\,\alpha \ge 0 \mid z + \alpha \,\Delta z \in (\mathbb{K})^*\bigr\}
\]
and use a suitable constant $\gamma\in(0,1)$ (e.g., $\gamma=0.99$) to define
\[
  \alpha_P 
    = \gamma \,\alpha_x, 
  \quad
  \alpha_D 
    = \gamma \,\alpha_z.
\]

\subsubsection{Computation of $\alpha_x$}
Below, we show the method for calculating $\alpha_x$.  
Let \[\alpha^p_x:=\sup\{\alpha \geq 0 \mid x^p + \alpha \Delta x^p \in \mathbb{K}^p\} \quad (p\in P)\]
then $\alpha_x=\min\{\alpha^p_x\mid p\in P\}$ holds.

\paragraph{Case 1: $\mathbb{K}^p = \mathbb{S}^{n_p}_+$.}

Using the Cholesky decomposition $x^p=LL^T$ of $x^p$,
\[\alpha^p_x = \sup\{\alpha \geq 0 \mid I + \alpha L^{-1} \Delta x^p (L^T)^{-1} \in \mathbb{K}^p\}\]
holds, thus if $\lambda_{\max}$ is the largest eigenvalue of $-L^{-1} \Delta x^p (L^T)^{-1}$,
\[\alpha^p_x = \begin{cases}
    1/\lambda_{\max} & \text{if} ~ \lambda_{\max} > 0 \\
    +\infty & \text{otherwise}
\end{cases}\]
is obtained.
$\lambda_{\max}$ can be efficiently calculated with good accuracy using methods such as the Lanczos method \cite{Golub2013}.

\paragraph{Case 2: $\mathbb{K}^p = \mathbb{Q}^{n_p}$.}

Using the following quadratic form
\begin{align*}
  f^p(\alpha^p)
  :&= ( x^p + \alpha^p \Delta x^p )^T J ( x^p + \alpha^p \Delta x^p )\\
   &=  ( x^p_0 + \alpha \Delta x^p_0 )^2 - \bigl\|( \bar{x}^p + \alpha \Delta \bar{x}^p )\bigr\|^2
\end{align*}
we can write
\begin{align*}
    \alpha_x^p 
    &= \sup\{\alpha \geq 0 \mid x^p + \alpha \Delta x^p \in \mathbb{Q}^{n_p}\}\\
    &= \sup\{\alpha \geq 0 \mid f^p(\alpha) \geq 0, ~x^p_0 + \alpha \Delta x^p_0 \geq 0\}.
\end{align*}

Consider the positive roots of the quadratic equation $f(\alpha)=0$. % List all positive roots and take the interval up to the maximum $\alpha$.
Expanding $f^p$,
\begin{align*}
    f^p(\alpha) = \alpha^2\underbrace{(\Delta x^p)^T J (\Delta x^p)}_a + 2 \alpha \underbrace{(x^p)^T J (\Delta x^p)}_b + \underbrace{(x^p)^T J x^p}_c
\end{align*}
where $d:=b^2-ac$.
Since $x^p\in \operatorname{int}(\mathbb{Q}^{n_p})$, $c = (x^p_0)^2 - \|\bar{x}^p\|^2 > 0$, the quadratic equation $f^p(\alpha) = 0$ has positive solutions in the following three cases:
\begin{enumerate}
    \item When $a<0$, $\alpha=\frac{-b-\sqrt{d}}{a}$ is the only positive solution.
    \item When $a=0$ and $b<0$, $\alpha=-\frac{c}{2b}$ is the only positive solution.
    \item When $a>0$, $b<0$, and $d\geq 0$, $\alpha=\frac{-b-\sqrt{d}}{a}$ is the smallest positive solution.
\end{enumerate}
These solutions always satisfy $x^p_0+\alpha \Delta x^p_0\geq 0$.
(\textbf{Proof:} If $\Delta x^p_0 \geq 0$, it is trivial. Consider the case $\Delta x^p_0 < 0$. First, $f^p(0)=c>0$. Also,
% Note that $f^p(\alpha) = (x^p_0 + \alpha \Delta x^p_0)^2 - \|(x^p + \alpha \Delta x^p)\|^2$,
$f^p(-\frac{x^p_0}{\Delta x^p_0}) = - \|(x^p -\frac{x^p_0}{\Delta x^p_0} \Delta x^p)\|^2 \leq 0$.
Thus, the smallest positive solution of $f^p(\alpha) = 0$ exists in the interval $\big(0, \frac{x^p_0}{-\Delta x^p_0}\big]$, and in this interval $x^p_0+\alpha \Delta x^p_0\geq 0$ always holds.)

Therefore,
\begin{equation*}
    \alpha^p_x=\begin{cases}
       \frac{-b - \sqrt{d}}{a} & \text{if } (a < 0) \text{ or } (a > 0 \text{ and } b < 0 \text{ and } d \geq 0)\\
       -\frac{c}{2b} & \text{if $a=0$} \\
       \infty & \text{otherwise}
    \end{cases}
\end{equation*}

\paragraph{Case 3: $\mathbb{K}^p = \mathbb{R}^{n_p}_+$.}
In this case,
\[
t_i= \begin{cases}
    x^p_i / (-\Delta x^p_i) & \text{if } \Delta x^p_i < 0 \\
    +\infty & \text{otherwise}
\end{cases}
\]
then,
\[
    \alpha^p_x = \min\{t_i \mid i=1,2,\ldots,n_p\}
\]
holds.

\paragraph{Case 4: $\mathbb{K}^p = \mathbb{R}^{n_p}$.}
In this case, there are no constraints, so $\alpha^p_x=+\infty$ is acceptable.

\medskip

\subsubsection{Computation of $\alpha_z$}
For $\alpha_z$, we calculate $\alpha^p_z = \sup\{\alpha\in [0, 1] \mid z^p + \alpha\Delta z^p \in (\mathbb{K}^p)^*\}$ for each block, and then $\alpha_z=\min\{\alpha^p_z \mid p \in P\}$.
Since $\mathbb{K}^p=\mathbb{S}^{n_p}_+,\mathbb{Q}^{n_p},\mathbb{R}^{n_p}_+$ are self-dual (i.e., $\mathbb{K}^p = \bigl(\mathbb{K}^p\bigr)^*$), $\alpha^p_z$ can be calculated in the same way as $\alpha^p_x$.
For $\mathbb{K}^p=\mathbb{R}^{n_p}$, since $\bigl(\mathbb{K}^p\bigr)^*=\{0\}^{n_p}$, $\Delta z^p = 0$ always holds, so $\alpha^p_z=+\infty$ is acceptable.

\medskip

\subsection{Initial Points}
\label{sec:initial_points}
The algorithms described in this paper can start from infeasible initial points, but the choice of initial points greatly affects the convergence speed and numerical stability of the iterations.
In practice, existing solvers (such as SDPT3) report that numerical calculations tend to become unstable when given initial points with extremely small or large norms \cite{toh1999}.
It is desirable to provide initial points with the same scale as the solutions of problems $(P),(D)$, and a simple and empirically effective initial point used in SDPT3 is $y = 0$, and for each $p\in P$,

\[
    x^p = \begin{cases}
        \zeta^p\, e^p, & \text{if } p \in P \setminus P^u,\\
        0, & \text{if } p\in P^u,
    \end{cases}
    \quad
    \quad 
    z^p = \begin{cases}
        \eta^p\, e^p, & \text{if } p \in P \setminus P^u,\\
        0, & \text{if } p\in P^u.
    \end{cases}
\]
is used as the initial point\footnote{If it is unknown whether problems $(P)$ and $(D)$ have feasible solutions or if they do not have any interior points, converting to a Homogeneous Self-Dual (HSD) model may be effective.
For example, in the 3-parameter HSD model proposed by Wright \cite{Wright1997},
auxiliary variables $(\tau, \kappa, \theta)$ are introduced to construct an extended problem, and applying interior-point methods to this can stabilize numerical calculations even when the feasibility is unknown.
However, this method may increase the number of iterations when starting from infeasible initial points, so it is often used depending on the scale and characteristics of the problem \cite{toh1999}.}. Here,
\begin{align*}
    \zeta^p 
    &= \max\Bigl\{
       10,\;\sqrt{n^p},\;\;\theta^p \max_{1 \le k \le m}\bigl\{\frac{1 + |b_k|}{1 + \|a^p_k\|}\bigr\}
      \Bigr\},
    \quad
    \quad
    \theta^p = \begin{cases}
        n^p, & \mathbb{K}^p=\mathbb{S}^{n_p}_+,\\
        \sqrt{n^p}, & \mathbb{K}^p=\mathbb{Q}^{n_p},\\
        1, & \text{otherwise},
    \end{cases}
    \\[6pt]
    \eta^p 
    &= \max\Bigl\{
       10,\;\sqrt{n^p},\;\max\{\|a^p_1\|,\ldots,\|a^p_m\|,\;\|c^p\|\}
      \Bigr\}.
\end{align*}

\medskip
\subsection{Stopping Criteria}
\label{sec:stopping_criteria}

In this section, we present the stopping criteria for terminating the iterations of the interior-point method.
SDPT3 terminates iterations when the predefined number of iterations or accuracy goals are met, or when infeasibility or numerical difficulties become apparent.

Specifically, we first define the following quantities to measure the dual gap and infeasibility:
\[
  \mathrm{gap}
  := \sum_{p\in P}
       \Bigl(\inprod{x^p}{z^p}_p + \bigl(\phi^p(x^p) - \phi^{p*}(z^p)\bigr)\Bigr),
\]
\[
  \mathrm{relgap}
  := \frac{\mathrm{gap}}
           {\,1 \;+\;\Bigl|\sum_{p\in P}\inprod{c^p}{x^p}_p\Bigr|
                 \;+\;|\,b^T y|\,},
\]
\[
  \mathrm{pinfeas}
  := \frac{\|\,R_{prim}\|}{\,1 + \|b\|\,},
  \quad
  \mathrm{dinfeas}
  := \frac{\sum_{p\in P}\|\,R^p_{dual}\|}
           {\,1 + \sum_{p\in P}\|\,c^p\|\,}.
\]
Refer to equation \eqref{eq:NewtonKKT} for the definitions of $R_{prim}$ and $R_{dual}^p$.
These represent the residuals of primal infeasibility and dual infeasibility, respectively.
If $\mathrm{pinfeas} = 0$ and $\mathrm{dinfeas} = 0$,
\[
  \sum_{p\in P}\inprod{x^p}{z^p}_p 
  \;=\;
  \sum_{p\in P}\inprod{x^p}{c^p - (\mathcal{A}^p)^T y}_p
  \;=\; \sum_{p\in P}\inprod{x^p}{c^p}_p \;-\; b^T y,
\]
holds, indicating that $\mathrm{gap}$ can be used as an indicator of the dual gap.

Under these definitions, the iterations are terminated when any of the following conditions are met:

\begin{enumerate}
    \item The number of iterations reaches the upper limit (default is 100 iterations).
    \item 
      $\displaystyle
      \max\{\,\mathrm{relgap},\;\mathrm{pinfeas},\;\mathrm{dinfeas}\}
      < \varepsilon
      $
      and the desired accuracy $\varepsilon$ is achieved.
    \item 
      $\displaystyle
        \frac{\,b^T y\,}
              {\,\sum_{p\in P}\|\,(\mathcal{A}^p)^T y + z^p\|\!}
      > \kappa
      $
      indicating that the primal problem (P) is likely infeasible.
    \item 
      $\displaystyle
      -\,\frac{\inprod{c}{x}}
              {\bigl\|\sum_{p\in P}\mathcal{A}^p x^p\bigr\|}
      > \kappa
      $
      indicating that the dual problem (D) is likely infeasible.
    \item Numerical errors occur:
      \begin{itemize}
          \item Failure in Cholesky decomposition of $x^p$ or $z^p$.
          \item Preconditioned iterative solver (such as SQMR) fails to converge.
          \item $\mathrm{gap}$ diverges, indicating erratic behavior.
      \end{itemize}
\end{enumerate}

Furthermore, it is practically important to heuristically terminate iterations when $\mathrm{relgap}$, $\mathrm{pinfeas}$, and $\mathrm{dinfeas}$ take relatively small values and there is little improvement in the last few iterations.
In SDPT3 \cite{toh1999}, various termination conditions are implemented in \texttt{sqlpcheckconvg.m}, such as terminating when relative errors and update amounts fall below certain thresholds.




\section{Predictor-Corrector Method}
In this chapter, we introduce the predictor-corrector method\footnote{The name predictor-corrector originates from numerical methods for ordinary differential equations.}.
This method is widely adopted in many software packages as an efficient way to solve problems.
The original method proposed by Mehrotra \cite{Mehrotra1992} is somewhat complex, so we introduce the version by Toh et al. \cite{toh1999} here.
Although the process differs slightly from the widely used algorithms (e.g., \cite{Wright1997}), the results obtained are equivalent.

In the predictor-corrector method, the search direction $(\Delta x, \Delta y, \Delta z)$ is computed in two stages.
Intuitively, in the first stage (predictor step), we compute a search direction $(\delta x, \delta y, \delta z)$ that reduces the duality gap.
Using this information, we determine a target point on the central path, and in the second stage (corrector step), we compute a search direction $(\Delta x, \Delta y, \Delta z)$ that pulls the solution back towards the central path.

\paragraph{Predictor Step}
Temporarily set $\sigma=0$, i.e., consider the case where
\[R^p_{\mathrm{comp}}=\nu^p \, e^p - (G^p x^p) \circ ((G^p)^{-1} z^p).\]
Let $(\delta x, \delta y, \delta z)$ be the solution to the equation \eqref{eq:NewtonKKT} in this case.
Using this predictor direction $(\delta x, \delta y, \delta z)$, compute the step sizes $\alpha_P, \alpha_D$ using the method in Section~\ref{sec:step_size}.
(This does not actually update the variables but temporarily determines $\alpha_P, \alpha_D$.)
Evaluate the estimated duality gap $\hat{\mu}$ using this predictor direction and step sizes, and determine the value of $\sigma$ to be used.
Specifically, with a parameter $\psi \ge 1$,
\[
   \sigma=\min\left\{1, \frac{\inprod{x + \alpha_P \delta x}{z + \alpha_D \delta z}}{\inprod{x}{z}}\right\}^\psi.
\]
Empirically, it is said that adopting $\psi=2,3,4$ is good, and SDPT3 uses a given parameter $\hat{\psi}=3$ to set
\[
\psi = \begin{cases}
    \max\{\hat{\psi}, 3 \min(\alpha_P, \alpha_D)^2\} & \text{if} ~ \mu > 10^{-6}, \\
    \max\{1, \min\{\hat{\psi}, 3 \min(\alpha_P, \alpha_D)^2\}\} & \text{otherwise}.
\end{cases}
\]
Here, $\mu$ is defined as in equation \eqref{eq:mu}.

\paragraph{Corrector Step}
Next, use the redefined $\sigma$. Furthermore, replace $R^p_{comp}$ in equation \eqref{eq:NewtonKKT} with
\[R^{p, corr}_{comp}=R^p_{comp}-(G^p \delta x^p)\circ((G^p)^{-1} \delta z^p),\]
and solve to obtain the final search direction $(\Delta x, \Delta y, \Delta z)$.
Compute the step sizes $\beta_P, \beta_D$ for this search direction using the method in Section~\ref{sec:step_size}.

\paragraph{Solution Update}
Update the next iteration point as $(x^+, y^+, z^+) = (x, y, z) + (\beta_P \Delta x, \beta_D \Delta y, \beta_D \Delta z)$.

In the equations to be solved between the predictor step and the corrector step, only $R^p_{\mathrm{comp}}$ changes.
That is, in the equation \eqref{eq:Schur_complement_Mat} to be solved, only the right-hand side vector $h$ changes.
Therefore, the coefficient matrix $\mathcal{M}$ computed in the predictor step, as well as its LU decomposition or Cholesky decomposition results, can be reused in the corrector step.
Moreover, for the value of $h$, let $h_{pred}$ be the one used in the predictor step and $h_{corr}$ be the one used in the corrector step.
Then,
\[h_{corr}=h_{pred} + \sum_{p\in P\setminus P^u} \mathcal{A}^p (\mathcal{E}^p)^{-1} \big((G^p \delta x^p) \circ ((G^p)^{-1} \delta z^p) \big),\]
so the computation result of the predictor step ($h_{pred}$) can be reused.
Thus, although introducing the predictor-corrector method increases the number of times the equation needs to be solved twice in succession, the increase in computational cost is relatively limited.


\medskip

\section{Sparsity Exploitation Technique} \label{sec:exploit_sparsity}
\subsection{Semidefinite cone} \label{sec:exploit_sparsity_sdp}
The dominant computational cost in the algorithms described so far arises from the calculation of $M^p$ when $\mathbb{K}^p = \mathbb{S}^{n_p}_+$. 
Naively calculating this requires repeating matrix multiplications twice and inner product calculations $O(m^2)$ times, making it crucial to leverage the sparsity of $a^p_k$ for practical efficiency. 
Here, we introduce a slightly modified idea proposed by Fujisawa et al.\cite{Fujisawa1997}.

\medskip

In the HKM direction, we have
\[
  M^p_{\sigma(i)\sigma(j)}
  = \inprod{\,a^p_{\sigma(i)}}{\,x^p\,a^p_{\sigma(j)}\,(z^p)^{-1}}
\]
Thus, it is important to note that we only need to calculate the elements corresponding to the non-zero elements of $a^p_{\sigma(i)}$, rather than all elements of $x^p\,a^p_{\sigma(j)}\,(z^p)^{-1}$.
Additionally, since $M^p$ is a symmetric matrix, we only need to calculate the upper triangular elements.

\medskip

Let $f_k$ denote the number of non-zero elements in $a^p_k$, and let $\sigma$ be a permutation of indices $1,\ldots,m$ such that $f_k$ is in ascending order.
For a fixed $j$, define the set
\[
  I
  := \bigl\{(\alpha,\beta)\mid (a^p_{\sigma(i)})_{\alpha\beta}\neq 0
         \text{ for some } i=1,\ldots,j \bigr\}
\]
We consider calculating $x^p\,a^p_{\sigma(j)}\,(z^p)^{-1}$ using one of the methods F1--F3, focusing on the number of multiplications required.

\begin{enumerate}
\item[F1:] First, calculate $F=a_{\sigma(j)}\,(z^p)^{-1}$ (requires $n\,f_{\sigma(j)}$ multiplications).
           Then, calculate $G=x^p\,F$ (requires $n^3$ multiplications).
           Thus, a total of $n\,f_{\sigma(j)} + n^3$ multiplications are required.
\item[F2:] Calculate $F=a_{\sigma(j)}\,(z^p)^{-1}$ (requires $n\,f_{\sigma(j)}$ multiplications). Then,
  \[
    G_{\alpha\beta}=
    \begin{cases}
     \sum_{\gamma=1}^m (x^p)_{\alpha\gamma}\,F_{\gamma\beta}, & \text{if } (\alpha,\beta)\in I,\\
     0, & \text{otherwise},
    \end{cases}
  \]
  (requires $n\,|I|$ multiplications).  
  Thus, a total of $n\,f_{\sigma(j)} + n\,|I|\kappa$ multiplications are required.
\item[F3:] Calculate each element of $G$ directly using the double sum:
  \[
    G_{\alpha\beta}=
    \begin{cases}
      \sum_{\gamma}\sum_{\delta}
        (x^p)_{\alpha\gamma}\,(a^p_{\sigma(j)})_{\gamma\delta}\,(z^p)^{-1}_{\delta\beta},
       & \text{if } (\alpha,\beta) \in I,\\
      0, & \text{otherwise},
    \end{cases}
  \]
  (requires $2f_{\sigma(j)}|I|$ multiplications).
\end{enumerate}

\noindent
F1 is naive and inefficient but has the advantage of utilizing highly optimized Basic Linear Algebra Subroutines (BLAS) for matrix multiplication.
F2 and F3 involve operations on scattered $(\alpha,\beta) \in I$, resulting in higher overhead for memory access, but F2 is advantageous when $|I| \ll n^2$, and F3 is advantageous when $f_{\sigma(j)} \ll n$.

SDPT3 uses...

\medskip

Although the method described here is for the HKM direction, similar techniques can be applied to the NT direction.

\subsection{Second-order and linear cones} \label{sec:exploit_sparsity_socp_lp}
When solving equation \eqref{eq:Schur_complement_Mat} using the methods described in Section~\ref{sec:direction},
if the matrices $\mathcal{M}$ and $M$ are sparse, we can speed up matrix-vector multiplications within the SQMR method and utilize highly optimized Cholesky decomposition and LU decomposition routines such as CHOLMOD, UMFPACK, and SparseLU.

In practical large-scale problems, $M$ is often a low-rank perturbation of a sparse positive definite matrix $M_{\mathrm{sparse}}$. This is because most $a^p_k$ are sparse, and only a few $a^p_k$ are dense.
Here, we introduce a \textbf{sparsity exploitation technique} that can be utilized in such cases.

Assume that for any $p\in P\setminus P^u$, there exist sufficiently small positive integers $n^p_+(\ll m)$ and matrices $M^p_{\mathrm{sparse}} \in \mathbb{S}^m_+$, $U^p \in \mathbb{R}^{m\times n^p_+}$, $D^p\in \mathbb{S}^{n^p_+}$ such that
\begin{equation}
  M^p = M^p_{\mathrm{sparse}} + U^p\, D^p\, (U^p)^T
  \label{eq:low_rank_perturbation}
\end{equation}
Let 
\begin{itemize}
\item $M_{\mathrm{sparse}} := \sum_{p\in P\setminus P^u} M^p_{\mathrm{sparse}}$,
\item $U := [U^p \text{horizontally concatenated}]$, 
\item $D:=[D^p\text{block diagonally concatenated}]$,
\end{itemize}
then the following linear equation
\begin{equation}
  \begin{pmatrix}
    M_{\mathrm{sparse}} & A^u & U \\
    (A^u)^T & O & O \\
    U^T & O & -D^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    \Delta y \\
    \Delta x^u \\
    \lambda
  \end{pmatrix}
  =
  \begin{pmatrix}
    h \\
    R^u_{dual} \\
    0
  \end{pmatrix}
  \label{eq:Schur_complement_Mat_aug}
\end{equation}
has a solution $\lambda = D\,U^T\,\Delta y$, indicating that equations \eqref{eq:Schur_complement_Mat_aug} and \eqref{eq:Schur_complement_Mat} are essentially equivalent.
The coefficient matrix in \eqref{eq:Schur_complement_Mat_aug} is a square matrix of dimension $m+\sum_{p\in P\setminus P^u} n^p$, which is larger than the $m$-dimensional square matrix in \eqref{eq:Schur_complement_Mat}, but has the advantage of being sparse.
This equation \eqref{eq:Schur_complement_Mat_aug} can be solved using the same methods as in Section~\ref{sec:direction}
(specifically, replacing $A^u$ with $[\,A^u, U]$ and $O$ with $\begin{pmatrix} O & O \\ O & -D^{-1} \end{pmatrix}$).

\medskip

Unfortunately, when $\mathbb{K}^p = \mathbb{S}^{n_p}_+$ for some $p \in P$, $M^p$ is often a dense matrix without specific structure, making it difficult to express in the form of \eqref{eq:low_rank_perturbation}.
On the other hand, if $\mathbb{K}^p \neq \mathbb{S}^{n_p}_+$ for any $p\in P$, it may be possible to reduce to \eqref{eq:Schur_complement_Mat_aug}.

\medskip

Below, we describe specific methods for constructing $M^p_{\mathrm{sparse}}$, $U^p$, and $D^p$ when $\mathbb{K}^p \neq \mathbb{S}^{n_p}_+$.
Let $A^p_{\mathrm{sparse}}$ be the matrix obtained by extracting the sparse columns of $A^p$, and $A^p_{\mathrm{dense}}$ be the matrix obtained by extracting the dense columns.
If $A^p_{\mathrm{dense}}$ is empty, let $M^p_{\mathrm{sparse}} = M^p$, and $U^p$, $D^p$ be empty matrices.
Below, we consider the case where $A^p_{\mathrm{dense}}$ is non-empty.

\paragraph{Second-order cone:}

When $\mathbb{K}^p=\mathbb{Q}^{n_p}$, using $-J = I - 2\,e^p(e^p)^T$, we have
\[
  M^p 
  = \bigl((x^p)^T J (z^p)^{-1}\bigr)\, A^p(A^p)^T
    \;+\; u^p (v^p)^T
    \;+\; v^p (u^p)^T
    \;-\; 2\,((x^p)^T J (z^p)^{-1})\, k^p (k^p)^T,
\]
where $u^p := A^p x^p$, $v^p := A^p (z^p)^{-1}$, $k^p := A^p e^p$.
Note that if $A^p_{\mathrm{dense}}$ is non-empty, $u^p, v^p$ are likely to be dense vectors.

Based on this, SDPT3 defines $M^p_{\mathrm{sparse}}$, $U^p$, and $D^p$ as follows:
\[
  M^p_{\mathrm{sparse}}
    := \bigl((x^p)^T J (z^p)^{-1}\bigr)\,
       A^p_{\mathrm{sparse}}\,(A^p_{\mathrm{sparse}})^T,
\]
\[
  U^p
    := \Bigl(\sqrt{(x^p)^T J (z^p)^{-1}}\;A^p_{\mathrm{dense}},
       \;\;\gamma(z^p)^2\,u^p,\;\;\gamma(z^p)^2\,v^p,
       \;-\sqrt{2\,((x^p)^T J (z^p)^{-1})}\,k^p\Bigr),
\]
\[
  D^p
    := \begin{pmatrix}
         I & O & O & O \\
         O & 0 & 1/\gamma(z^p)^2 & 0 \\
         O & 1/\gamma(z^p)^2 & 0 & 0 \\
         O & 0 & 0 & -1
       \end{pmatrix}.
\]
In practice, $M^p = M^p_{\mathrm{sparse}} + U^p D^p (U^p)^T$ holds, and introducing $k^p$ ensures that $M^p_{\mathrm{sparse}}$ is a positive definite matrix.
Note that in implementation, $D^p$ is not directly handled, only $-(D^p)^{-1}$ is needed, which can be written as
\[
  -(D^p)^{-1}
  = \begin{pmatrix}
      -I & O & O & O \\
      O & 0 & -\gamma(z^p)^2 & 0 \\
      O & -\gamma(z^p)^2 & 0 & 0 \\
      O & 0 & 0 & 1
    \end{pmatrix}
\]

\paragraph{Linear cone:}

When $\mathbb{K}^p = \mathbb{R}^{n_p}_+$, it is even simpler.
Let $x^p_{\mathrm{sparse}}, z^p_{\mathrm{sparse}}$ be the vectors obtained by extracting the elements of $x^p, z^p$ corresponding to the sparse columns of $A^p$, and $x^p_{\mathrm{dense}}, z^p_{\mathrm{dense}}$ be the vectors obtained by extracting the elements corresponding to the dense columns.
SDPT3 defines $M^p_{\mathrm{sparse}}$, $U^p$, and $D^p$ as follows:
\[
   M^p_{\mathrm{sparse}}
   = A^p_{\mathrm{sparse}}
     \,\operatorname{diag}(x^p_{\mathrm{sparse}})
     \,\operatorname{diag}(z^p_{\mathrm{sparse}})^{-1}
     \,(A^p_{\mathrm{sparse}})^T,
\]
\[
   U^p
   = \operatorname{diag}(x^p_{\mathrm{sparse}})^{\tfrac12}
     \,\operatorname{diag}(z^p_{\mathrm{sparse}})^{-\tfrac12}
     \,A^p_{\mathrm{dense}},
\]
\[ D^p = I, \]
In practice, $M^p = M^p_{\mathrm{sparse}} + U^p D^p (U^p)^T$ holds, and $M^p_{\mathrm{sparse}}$ is a positive definite matrix.
In this case,
\[-(D^p)^{-1} = -I\]

\medskip

\section{Other computation techniques}

\subsection{Perturbation of \boldmath $M$ }

Theoretically, if $\mathcal{A}$ is full rank, $M$ will be positive definite. However, in practice, due to numerical errors or inputs where $\mathcal{A}$ is not full rank, $M$ may not be a positive definite matrix, and Cholesky decomposition may fail.  
To avoid such numerical difficulties, SDPT3 applies a small perturbation to $M$ using sufficiently small $\epsilon > 0$ and $\lambda > 0$:
\[
  M \;\leftarrow\; M + \epsilon I + \lambda \sum_{p\in P} A^p (A^p)^T
\]

\subsection{Handling free variables}
If there exists any $p \in P$ such that $\nu^p = 0$, the problem (P)(D) can be transformed into the following 3-parameter Homogeneous self-dual (HSD) model \cite{Wright1997}:
\[
  \begin{array}{cl}
   \min_{x,y,z,\tau,\kappa,\theta} & \bar{\alpha}\,\theta \\[3pt]
   \text{s.t.}
   & \begin{pmatrix}
       0 & -\mathcal{A} & b & -\bar{b}\\
       \mathcal{A}^T & 0 & -c & \bar{c}\\
       -b^T & c^T & 0 & -\bar{g}\\
       \bar{b}^T & -\bar{c}^T & \bar{g} & 0
     \end{pmatrix}
     \begin{pmatrix} y \\ x \\ \tau \\ \theta \end{pmatrix}
   \;+\;
     \begin{pmatrix} 0 \\ z \\ \kappa \\ 0 \end{pmatrix}
   =
     \begin{pmatrix} 0 \\ 0 \\ 0 \\ \bar{\alpha} \end{pmatrix}.
  \end{array}
\]
Given $(x_0,y_0,z_0,\tau_0,\kappa_0,\theta_0)\in 
  \operatorname{int}(\mathbb{K}) \times \mathbb{R}^m \times \operatorname{int}(\mathbb{K})
  \times \mathbb{R}^1_+ \times \mathbb{R}^1_+ \times \mathbb{R}^1_+$
we define
\begin{gather*}
    \bar{b} = \frac{1}{\theta_0}(b\tau_0 - \mathcal{A}x_0) \\
    \bar{c} = \frac{1}{\theta_0}(c\tau_0 - \mathcal{A}^T y_0 - z_0) \\
    \bar{g} = \frac{1}{\theta_0}(\inprod{c}{x_0} - b^Ty_0 + \kappa_0) \\
    \bar{\alpha} = \frac{1}{\theta_0} (\inprod{x_0}{z_0} + \tau_0 \kappa_0)
\end{gather*}
According to Toh et al.~\cite{toh1999}, if the feasible region of (P)(D) is non-empty but does not contain an interior point, converting to the HSD model and then applying the interior-point method results in longer computation times but yields better accuracy. This approach is implemented in SDPT3.  
Handling the HSD model requires slight modifications to the algorithm, detailed in \cite{toh1999}.

\medskip

On the other hand, if there exists any $p \in P$ such that $\nu^p \neq 0$, unfortunately, (P)(D) cannot be converted to the HSD model.
As seen from \eqref{eq:Schur_complement_Mat}, if there exists any $p \in P$ such that $\mathbb{K}^p = \mathbb{R}^{n_p}$, the presence of $A^u$ increases the size of the equation to be solved, making it computationally expensive.
Therefore, SDPT3 eliminates the free variable $x^p \in \mathbb{R}^{n_p}$ by transforming it into non-negative variables $(x^p_+,\,x^p_-)\in\mathbb{R}^{2n_p}_+$:
\[
   x^p = x^p_+ \;-\; x^p_- 
\]

The downside of this method is numerical instability. In practice, $x^p_+$ and $x^p_-$ tend to become very large as iterations progress, while the corresponding dual variables $z^p_+, z^p_-$ tend to become very small.
This results in $\operatorname{diag}(x^p_\pm)\,\operatorname{diag}(z^p_\pm)$ becoming extremely ill-conditioned.
Fortunately, heuristic update formulas can be applied to suppress the increase in $x^p_+$ and $x^p_-$ values, mitigating numerical instability:
\[
   x^p_+ \;\leftarrow\; x^p_+ - 0.8\,\min(x^p_+,\,x^p_-),
   \quad
   x^p_- \;\leftarrow\; x^p_- - 0.8\,\min(x^p_+,\,x^p_-).
\]
Additionally, adding a positive perturbation of the same scale as $\mu$ to $z^p_+, z^p_-$ at each iteration prevents them from becoming excessively small.

\medskip

Even for blocks $p$ where $\mathbb{K}^p=\mathbb{R}^{n_p}_+$, if there exist pairs of non-negative variables that can be transformed into free variables (e.g., if there exist pairs of variables $x^p_i, x^p_j$ such that $(a^p_k)_i = -(a^p_k)_j$ for all $k=1,2,\ldots,m$, $(c^p)_i = -(c^p)_j$, $\nu^p = 0$), similar numerical instability occurs. Therefore, it is necessary to detect such pairs of variables and apply the heuristic update mentioned above.

\subsection{Preprocessing for model transformation}

\subsubsection{Complex input}

In applications such as control engineering, optimization problems on complex semidefinite cones may arise, but they can be reduced to problems on real semidefinite cones.
Define the following:
\begin{itemize}
    \item Complex matrix space $\mathbb{C}^{m\times n}$
    \item Set of Hermitian matrices $\mathbb{H}^n=\{a \in \mathbb{C}^{n\times n} \mid a = a^H\}$ where $a^H$ denotes the conjugate transpose of $a$.
    \item Complex positive semidefinite symmetric matrix cone $\mathbb{H}^n_+=\{a \in \mathbb{H}^n \mid x^T a x \geq 0 ~(\forall x\in \mathbb{C}^n)\}$
    \item $\bar{\mathbb{S}}^{2n}_+ = \{(\begin{smallmatrix}
    A & B\\
    C & D
\end{smallmatrix}) \in \mathbb{S}^{2n}_+ \mid A=D\in \mathbb{S}^n, ~ B=-B^T=C=-C^T\}$
\end{itemize}
It is known that $\mathbb{H}^n_+$ and $\bar{\mathbb{S}}^{2n}_+$ are algebraically isomorphic.
Define the mapping $\Gamma: \mathbb{H}^n\to \mathbb{S}^{2n}$ as
\[
  \Gamma(x)
  := \begin{pmatrix}
       \operatorname{real}(x) & -\operatorname{imag}(x) \\
       \operatorname{imag}(x) & \operatorname{real}(x)
     \end{pmatrix}
\]
For example,
\[
  x\in \mathbb{H}^n_+
   \;\;\Longleftrightarrow\;\;
  \Gamma(x)\in \mathbb{S}^{2n}_+
\]
Thus, by transforming variables using $\Gamma(x)= y \in \bar{\mathbb{S}}^{2n_p}_+$, problems on complex semidefinite cones can be reduced to problems on real semidefinite cones.

\subsubsection{Detect diagonal block of $\mathbb{K}^p=\mathbb{S}^{n_p}_+$}

The condition $x\in \mathbb{S}^1_+$ is equivalent to $x\in \mathbb{R}^1_+$.
In this case, treating $x$ as a simple non-negative real variable improves the computational efficiency of the interior-point method.
Furthermore, if there are diagonal blocks within the variable matrix of $\mathbb{S}^{n_p}_+$, converting them to non-negative real variables yields similar computational efficiency improvements.

Specifically, for an integer $i$,
if $(c^p)_{ij}=(c^p)_{ji}=0$ and $(a^p_k)_{ij}=(a^p_k)_{ji}=0$ 
($\forall j\neq i,\,\forall k$),
transform $x\in \mathbb{S}^{n_p}_+$ to $(\bar{x},\,\hat{x})\in \mathbb{S}^{n_p-1}_+\times \mathbb{R}^1_+$.
Here, $\bar{x}$ is the matrix obtained by removing the $i$-th row and column from $x$, and $x_{ij}= x_{ji}=0$ ($\forall j\neq i$).

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 0 & 1 \\
            0 & 5 & 0 \\
            1 & 0 & 2
        \end{pmatrix}}{X} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 \\ 
            0 & 2 & 0 \\ 
            0 & 0 & 3
        \end{pmatrix}}{X} = 1 \\
        & X\in \mathbb{S}^3_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 1 \\
            1 & 2
        \end{pmatrix}}{\bar{X}} + 5\hat{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 0 & 3
        \end{pmatrix}}{\bar{X}} + 2\hat{x} = 1 \\
        & \bar{X}\in \mathbb{S}^2_+, \quad \hat{x} \in \mathbb{R}^1_+
    \end{array}
    \right.
\end{equation*}
\end{example}
Indeed, in SDPLIB's xxx, there are many such diagonal blocks.

\subsubsection{Artificial linear block}

The interior-point method algorithm introduced in this paper assumes $m \geq 1$ and $\exists p\in P ~ \text{s.t.} ~ \nu^p=0$ (mainly for \eqref{eq:NewtonKKT} and \eqref{eq:mu}).

If these conditions are not met, adding one redundant non-negative variable $x^{p_{\max} + 1}$ and one constraint
\[
  -\sum_{p\in P} \inprod{e^p}{x^p}_p + x^{p_{\max} + 1} = 0
\]
can satisfy the assumptions.
Specifically, set new parameters as follows:
\begin{itemize}
    \item $\mathbb{K}^{p_{\max} + 1} = \mathbb{R}^1_+$
    \item $a^{p}_1=e^p ~ (p \in P)$, $a^{p_{\max} + 1}_1 = 1$
    \item $b = 0$
    \item $c^{p_{\max} +1}=0$
    \item $\nu^{p_{\max} + 1} = 0$
\end{itemize}

\subsubsection{Reordering Matrix Variables}
To determine the positive definiteness of variables $x^p, z^p$, checking whether Cholesky decomposition succeeds is numerically stable.
Since the interior-point method performs this Cholesky decomposition at each iteration, minimizing fill-in is important.
SDPT3 reorders matrix variables to minimize fill-in during Cholesky decomposition.
Specifically, define
\[
  t^p = |c^p| + \sum_{k=1}^m |a^p_k|
\]
and apply the Reverse Cuthill--Mckee Algorithm to obtain the permutation $\sigma$.
Finally, transform variables as $(\bar{x}^p)_{ij} = (x^p)_{\sigma(i)\sigma(j)}$.

\begin{example}
\[
    t^p = \begin{pmatrix}
        3 & 0 & 1 \\\
        0 & 5 & 0 \\\
        1 & 0 & 2
    \end{pmatrix}
\] Applying the Reverse Cuthill-Mckee Algorithm yields $\sigma(1)=3, ~ \sigma(2)=1, ~ \sigma(3)=2$.
Transforming variables using this permutation results in $\bar{t}^p$:
\[
    \bar{t}^p = \begin{pmatrix}
        3 & 1 & 0 \\
        1 & 2 & 0 \\
        0 & 0 & 5
    \end{pmatrix}
\]
which indeed has a smaller bandwidth.
\end{example}

\subsubsection{Utilizing matrix symmetry}
In blocks where $\mathbb{K}^p=\mathbb{S}_+^p$, leveraging the symmetry of coefficient matrices can reduce memory usage and computation time for matrix inner products.

Given a real symmetric matrix $A\in \mathbb{S}^n$, define $\operatorname{svec}: \mathbb{S}^n \rightarrow \mathbb{R}^{n(n+1)/2}$ for $i\leq j$ as follows:
\[ \operatorname{svec}(A)= (a_1^T, a_2^T, \ldots a_n^T)^T \]
where
\[ a_j = (f_{j,j} A_{j,j}, ~ f_{j+1,j}A_{j+1,j}, ~ \ldots ~ , ~ f_{n,j} A_{n,j})^T \in \mathbb{R}^{n-j+1} \]
\[ f_{ij}= \begin{cases} 1 & \text{if} ~ i=j \\ \sqrt{2} & \text{otherwise} \end{cases}\]

\begin{example}
\[A = \begin{pmatrix}
    1 & 2 & 3\\
    2 & 4 & 5\\
    3 & 5 & 6
\end{pmatrix} \rightarrow \operatorname{svec}(A) = (1, ~ 2\sqrt{2}, ~ 3\sqrt{2}, ~ 4, ~ 5\sqrt{2}, ~ 6)^T\]
\end{example}

The following equality holds:
\[\inprod{A}{A} = \operatorname{svec}(A)^T \operatorname{svec}(A).\]
Thus, by storing $\operatorname{svec}(A)$ instead of $A$, memory usage and computation time for inner products can be approximately halved.
SDPT3 adopts the $\operatorname{svec}$ format for storing coefficient matrices $a^p_k$ (while $c^p$, $x^p$, and $z^p$ are stored in matrix form).




\section{Summary}
The pseudo-code for the primal-dual path-following infeasible-interior-point method incorporating various techniques discussed so far is as follows:
TBA
% \begin{algorithm}
% Primal-dual path-following infeasible-interior-point method
% \begin{algorithmic}[1]
% \REQUIRE $\sigma\in[0,1)$
% \STATE Initialize $x,y,z$
% \STATE xxxxxxxxxxxxxx
% \FOR{it in $1,2,\ldots,\text{maxit}$}
%     \STATE Compute the coefficient matrices $M$, $A^u$, $U$, $-D^{-1}$ of equation \eqref{eq:Schur_complement_Mat_aug}
%     \STATE Compute the Cholesky decomposition of $M$ and the LU decomposition of $S$
%     \STATE Compute $\mathcal{A}\mathcal{H}R_{dual}$ and $R^u_{dual}$
%     % \STATE Compute sigma \& mu
%     \STATE Compute $R_{comp}$
%     \STATE Compute $\mathcal{A}\mathcal{E}^{-1}R_{comp}$
%     \STATE Compute $h = R_{prim} + \mathcal{A}\mathcal{E}^{-1}R_{comp} - \mathcal{A}\mathcal{H}R_{dual}$
%     \STATE Solve equation \eqref{eq:Schur_complement_Mat_aug} to obtain $\delta y, \delta x^u$
%     \STATE Obtain $\delta x, \delta z$ from equation \eqref{eq:sol_x_z}
%     \STATE Compute $\mu$ and $\sigma$
%     % \STATE heuristic stopping criteria
%     \STATE Compute $R_{comp}$ according to equation \eqref{}
%     \STATE Compute $h$ according to equation \eqref{}
%     \STATE 
%     \STATE Compute dX, dZ
%     \STATE ... TBA
%     \IF{$x,y,z$ satisfies the conditions}
%         \RETURN x,y,z
%     \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}


\appendix
\section{A Guide to the Derivation of Equations \eqref{eq:HKM_HRd}--\eqref{eq:NT_M}} \label{sec:guide_for_dir_eq}

% \begin{equation}
%     \everymath{\displaystyle}
%     \renewcommand{\arraystretch}{2.5}
%     \begin{array}{rl}
%        & \sum_{p=1}^P \mathcal{A}^p(\Delta x^p) = R_{prim} \\
%        \Leftrightarrow 
%        & \sum_{p = u} \mathcal{A}^p(\Delta x^p) + \sum_{p \neq u} \mathcal{A}^p(\Delta x^p) = R_{prim} \\
%        \Leftrightarrow 
%        & \sum_{p = u} \mathcal{A}^p((\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p(R_{dual}^p - \mathcal{A}^p(\Delta y))) + \sum_{p \neq u} \mathcal{A}^p(\Delta x^p) = R_{prim} \\
%        \Leftrightarrow 
%        & \sum_{p = u} \mathcal{A}^p\mathcal{H}^p(\mathcal{A}^p)^T\Delta y = h :=  R_{prim} - \sum_{p = u} \mathcal{A}^p((\mathcal{E}^p)^{-1}R_{comp}^p - \mathcal{H}^p R_{dual}^p) - \sum_{p \neq u} \mathcal{A}^p(\Delta x^p)
%     \end{array}
% \end{equation}

% \begin{align*}
% a>0 \quad \text{and} \quad c>0 
% & \Rightarrow (\Delta x^p)_0 > \|\Delta \bar{x}^p\| \quad \text{and} \quad (x^p)_0 > \|\bar{x}^p\| \\
% & \Rightarrow (\Delta x^p)_0 (x^p)_0 > \|\Delta \bar{x}^p\|\|\bar{x}^p\| \geq (\Delta \bar{x}^p)^T \bar{x}^p \\
% & \Rightarrow b = (\Delta x^p)_0 (x^p)_0 - (\Delta \bar{x}^p)^T \bar{x}^p > 0
% \end{align*}

In the case of $\mathbb{K}^p=\mathbb{S}_+^{n_p}$, the derivation for the HKM direction is straightforward by noting that $G^p=(z^p)^\frac{1}{2}$ and $\mathcal{E}\Delta x = ((z^p)^\frac{1}{2}\Delta x^p)\circ ((z^p)^{-\frac{1}{2}}z^p)=(z^p)^\frac{1}{2} \Delta x^p (z^p)^\frac{1}{2}$. For the NT direction, the derivation is non-trivial and can be found in \cite{todd1998}.
In the case of $\mathbb{K}^p=\mathbb{Q}^{n_p}$, for $f\in \mathbb{R}^{n^p}$, %define the linear operator $Arw: \mathbb{R}^{n_p} \rightarrow \mathbb{R}^{n\times n}$ as
\begin{equation}
    \operatorname{Arw}(f) = 
    \left(
    \begin{array}{cc}
        f_0 & \bar{f}^T \\
        \bar{f} & f_0 I
    \end{array}
    \right)
\end{equation}
then $x^p \circ z^p = \operatorname{Arw}(x^p)z^p$ holds, so
\begin{align*}
    (\mathcal{E}^p)^{-1}R^p_{comp} &= G^p \cdot \operatorname{Arw}(G^p x^p)^{-1} \cdot R^p_{comp}\\
    \mathcal{H}^p R^p_{dual} &= G^p \cdot \operatorname{Arw}(G^p x^p)^{-1} \cdot \operatorname{Arw}\big((G^p)^{-1} z^p\big) \cdot(G^p)^{-1} \cdot R^p_{dual}\\
    \mathcal{A}^p\mathcal{H}^p(\mathcal{A}^p)^T &= A^p \cdot G^p \cdot \operatorname{Arw}(G^p x^p)^{-1} \cdot \operatorname{Arw}\big((G^p)^{-1} z^p\big) \cdot(G^p)^{-1} \cdot (A^p)^T
\end{align*}
can be generally described. Furthermore,
\begin{equation*}
    \operatorname{Arw}(f)^{-1} = \frac{1}{\gamma(f)^2} \left(\begin{array}{cc}
        f_0 & -\bar{f}^T \\
        -\bar{f} & \frac{1}{f_0}(\gamma(f)^2 I + \bar{f}\bar{f}^T)
    \end{array}\right),
    \qquad
    (G^p)^{-1} = \frac{1}{\omega^p} \begin{pmatrix}
        t^p_0 & -(\bar{t}^p)^T \\
        -\bar{t}^p & I+\frac{1}{1 + t^p_0} \bar{t}^p(\bar{t}^p)^T
    \end{pmatrix}
\end{equation*}
holds (note that $\gamma(t)=1$ is assumed), which can be used to derive equations \eqref{eq:HKM_HRd}--\eqref{eq:NT_M}.
In the case of $\mathmath{K}^p=\mathbb{R}^{n_p}_+$, the derivation is straightforward by noting that $\mathcal{E}^p \Delta x^p = \operatorname{diag}(z^p) \Delta x^p$.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
