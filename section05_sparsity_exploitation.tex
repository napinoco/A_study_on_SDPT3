\section{Sparsity Exploitation Technique} \label{sec:exploit_sparsity}
\subsection{Semidefinite cone} \label{sec:exploit_sparsity_sdp}
The dominant computational cost in the algorithms described so far arises from the calculation of $M^p$ when $\mathbb{K}^p = \mathbb{S}^{n_p}_+$. 
Since each element of $M^p$ requires matrix multiplications and an inner product, computing all $O(m^2)$ elements naively becomes computationally expensive, making it crucial to leverage the sparsity of $a^p_k$ for practical efficiency. 
SDPT3 implements a modified version of the technique proposed by Fujisawa et al.\cite{Fujisawa1997}. Here, we present this approach with further reorganization for clarity of exposition.

\medskip

Recall that, in the HKM direction, we have
\[
  M^p_{ij}
  = \inprod{a^p_i}{x^p\,a^p_j\,(z^p)^{-1}}
\]
Let $j$ be fixed. Observe that:
\begin{enumerate}
\item[(i)] Once we compute $G = x^p\,a^p_j\,(z^p)^{-1}$, it can be reused to calculate $M^p_{ij} = \langle a^p_i, G \rangle$ for all $i$.
\item[(ii)] Since $M^p$ is symmetric, we only need to compute the upper triangular part (i.e., $M^p_{ij}$ for $i \leq j$).
\item[(iii)] For computing $M^p_{ij}$, we only need the elements of $G$ corresponding to the non-zero positions of $a^p_i$ (where $i \leq j$), rather than all elements of $G$.
\end{enumerate}

To leverage property (iii) efficiently, we introduce a permutation $\sigma$ of indices $1,\ldots,m$ such that $f_{\sigma(k)}$ is in ascending order, where $f_k$ denotes the number of non-zero elements in $a^p_k$.

For a fixed $j$, define the index set
\[
  I
  := \bigl\{(\alpha,\beta)\mid (a^p_{\sigma(i)})_{\alpha\beta}\neq 0
         \text{ for some } i=1,\ldots,j \bigr\}
\]
representing all matrix positions needed for inner products with $a^p_{\sigma(i)}$ ($i \leq j$).
Computing $G = x^p\,a^p_{\sigma(j)}\,(z^p)^{-1}$ requires only the elements $G_{\alpha\beta}$ where $(\alpha,\beta) \in I$. By processing matrices in order of increasing density, the size of $I$ tends to grow gradually.

We now describe three methods (F1--F3) for computing these required elements, evaluating their computational costs by counting the number of multiplications:
\begin{enumerate}
\item[\textbf{F1:}] First, calculate $F=a^p_{\sigma(j)}\,(z^p)^{-1}$, which requires $n_p f_{\sigma(j)}$ multiplications.
           Then, calculate $G=x^p F$, which requires $n_p^3$ multiplications.
           Thus, a total of $n_p f_{\sigma(j)} + n_p^3$ multiplications are required.
\item[\textbf{F2:}] First, calculate $F=a^p_{\sigma(j)}\,(z^p)^{-1}$, which requires $n_p f_{\sigma(j)}$ multiplications. Then compute:
  \[
    G_{\alpha\beta}=
    \begin{cases}
     \sum_{\gamma=1}^{n_p} (x^p)_{\alpha\gamma} \, F_{\gamma\beta}, & \text{if } (\alpha,\beta)\in I,\\
     0, & \text{otherwise},
    \end{cases}
  \]
  This computation requires $n_p$ multiplications for each of the $|I|$ elements.
  Thus, a total of $n_p (f_{\sigma(j)} + |I|)$ multiplications are required.
\item[\textbf{F3:}] Compute each element of $G$ directly as follows:
  \[
    G_{\alpha\beta}=
    \begin{cases}
      \sum_{(\gamma,\delta): (a^p_{\sigma(j)})_{\gamma\delta} \neq 0}\,
        (x^p)_{\alpha\gamma} \, (a^p_{\sigma(j)})_{\gamma\delta} \, (z^p)^{-1}_{\delta\beta},
       & \text{if } (\alpha,\beta) \in I,\\
      0, & \text{otherwise}.
    \end{cases}
  \]
  Since each non-zero element of $a^p_{\sigma(j)}$ contributes to the computation, this requires $2f_{\sigma(j)}$ multiplications for each of the $|I|$ elements.
  Thus, a total of $2f_{\sigma(j)}|I|$ multiplications are required.
\end{enumerate}
F1 is straightforward but computationally expensive, though it benefits from highly optimized BLAS routines for dense matrix multiplication.
F2 and F3 compute only the necessary elements $(\alpha,\beta) \in I$, which can lead to significant savings when $|I| \ll n_p^2$, despite some overhead from less cache-friendly memory access patterns compared to BLAS operations.
Specifically, F2 is efficient when the set $I$ is small relative to the matrix size (i.e., $|I| \ll n_p^2$), while F3 excels when the matrix $a^p_{\sigma(j)}$ is highly sparse (i.e., $f_{\sigma(j)} \ll n_p$).

In SDPT3, the choice between methods F1--F3 for each $j$ is determined during a preprocessing phase, based on the sparsity patterns and expected computational costs.

Although the methods described here are presented for the HKM direction, the same sparsity exploitation techniques apply to the NT direction.


\subsection{Second-order and linear cones} \label{sec:exploit_sparsity_socp_lp}
When solving equation \eqref{eq:Schur_complement_Mat} using the methods described in Section~\ref{sec:direction},
if the matrices $\mathcal{M}$ and $M$ are sparse, we can speed up matrix-vector multiplications within the SQMR or BiCGSTAB methods and utilize highly optimized Cholesky decomposition and LU decomposition routines such as CHOLMOD and UMFPACK.

Unfortunately, in practical large-scale problems, $\mathcal{M}$ and $M$ are often dense matrices. 
However, $M$ can often be expressed as a sparse positive (semi-)definite matrix $M_{\mathrm{sparse}}$ plus a low-rank perturbation.
This structure arises because most constraint vectors $a^p_k$ are sparse, with only a few being dense.
Here, we introduce a sparsity exploitation technique that can be utilized in such cases.

Assume that for any $p\in P\setminus P^{\text{u}}$, the matrix $M^p$ can be decomposed as
\begin{equation}
  M^p = M^p_{\mathrm{sparse}} + U^p\, D^p\, (U^p)^T
  \label{eq:low_rank_perturbation}
\end{equation}
where $M^p_{\mathrm{sparse}} \in \mathbb{S}^m_+$ is a sparse positive semidefinite matrix, 
$U^p \in \mathbb{R}^{m\times n^p_+}$ with $n^p_+ \ll m$ (low rank), 
and $D^p\in \mathbb{S}^{n^p_+}$.

By aggregating these decompositions, we define:
\begin{align}
  M_{\mathrm{sparse}} &:= \sum_{p\in P\setminus P^{\text{u}}} M^p_{\mathrm{sparse}}, \\
  U &:= [U^p ~ (p\in P) \text{ concatenated horizontally}], \\
  D &:= [D^p ~ (p\in P) \text{ concatenated block diagonally}].
\end{align}

Using this decomposition, we can transform the original system \eqref{eq:Schur_complement_Mat} 
into an augmented sparse system. Specifically, the following linear equation
\begin{equation}
  \begin{pmatrix}
    M_{\mathrm{sparse}} & A^{\text{u}} & U \\
    (A^{\text{u}})^T & O & O \\
    U^T & O & -D^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    \Delta y \\
    \Delta x^{\text{u}} \\
    \lambda
  \end{pmatrix}
  =
  \begin{pmatrix}
    h \\
    R^{\text{u}}_{dual} \\
    0
  \end{pmatrix}
  \label{eq:Schur_complement_Mat_aug}
\end{equation}
is equivalent to \eqref{eq:Schur_complement_Mat} with the auxiliary variable $\lambda = D\,U^T\,\Delta y$.

The augmented coefficient matrix has dimension $m+\sum_{p\in P\setminus P^{\text{u}}} n^p_+$, 
which is larger than the original $m \times m$ system. 
However, it offers the computational advantage of being sparse, 
enabling the use of efficient sparse linear algebra routines.

This augmented system \eqref{eq:Schur_complement_Mat_aug} can be solved using the methods 
described in Section~\ref{sec:direction} with 
$A^{\text{u}}$ replaced by $[\,A^{\text{u}}, U]$ and the zero blocks appropriately replaced by 
$\begin{pmatrix} O & O \\ O & -D^{-1} \end{pmatrix}$.

Unfortunately, when $\mathbb{K}^p = \mathbb{S}^{n_p}_+$ for some $p \in P$, 
$M^p$ is often a dense matrix without exploitable structure, 
making it difficult to express in the form of \eqref{eq:low_rank_perturbation}.
However, when $\mathbb{K}^p = \mathbb{Q}^{n_p}$ or $\mathbb{K}^p = \mathbb{R}^{n_p}_+$, 
we can construct the decomposition \eqref{eq:low_rank_perturbation} 
and utilize the augmented system \eqref{eq:Schur_complement_Mat_aug}.

We now describe specific methods for constructing $M^p_{\mathrm{sparse}}$, $U^p$, and $D^p$ 
for these cases.
First, partition $A^p$ into sparse and dense columns:
$A^p_{\mathrm{sparse}}$ contains the sparse columns and 
$A^p_{\mathrm{dense}}$ contains the dense columns.
In SDPT3, this partition is based on the sparsity ratio of each column;
a column is classified as sparse if its ratio of non-zero elements 
is below a certain threshold (0.4 by default).
If $A^p_{\mathrm{dense}}$ is empty, we simply set $M^p_{\mathrm{sparse}} = M^p$, 
with $U^p$ and $D^p$ as empty matrices.
Otherwise, we construct the decomposition as follows.


\paragraph{Second-order cone:}
Consider $\mathbb{K}^p=\mathbb{Q}^{n_p}$. 
Using the identity $-J = I - 2\,e^p(e^p)^T$, 
the expression for $M^p$ from Section~\ref{sec:direction} can be rewritten as
\[
  M^p 
  = \bigl((x^p)^T J (z^p)^{-J}\bigr)\, A^p(A^p)^T
    \;+\; u^p (v^p)^T
    \;+\; v^p (u^p)^T
    \;-\; 2\,((x^p)^T J (z^p)^{-J})\, k^p (k^p)^T,
\]
where $u^p := A^p x^p$, $v^p := A^p (z^p)^{-J}$, and $k^p := A^p e^p$.
When $A^p_{\mathrm{dense}}$ is non-empty, the vectors $u^p$ and $v^p$ become dense, 
making $M^p$ a dense matrix.

To exploit sparsity, SDPT3 decomposes $M^p$ by separating the contributions 
from sparse and dense columns. Specifically, it defines:
\[
  M^p_{\mathrm{sparse}}
    := \bigl((x^p)^T J (z^p)^{-J}\bigr)\,
       A^p_{\mathrm{sparse}}\,(A^p_{\mathrm{sparse}})^T,
\]
\[
  U^p
    := \Bigl(
       \sqrt{(x^p)^T J (z^p)^{-J}}\;A^p_{\mathrm{dense}},
       \;\; u^p,
       \;\; \gamma(z^p)^2\,v^p,
       \;\; -\sqrt{2\,((x^p)^T J (z^p)^{-J})}\,k^p
    \Bigr),
\]
\[
  D^p
    := \begin{pmatrix}
         I & O & O & O \\
         O & 0 & 1/\gamma(z^p)^2 & 0 \\
         O & 1/\gamma(z^p)^2 & 0 & 0 \\
         O & 0 & 0 & -1
       \end{pmatrix}.
\]
This decomposition satisfies $M^p = M^p_{\mathrm{sparse}} + U^p D^p (U^p)^T$, 
where the inclusion of the $k^p$ term ensures that $M^p_{\mathrm{sparse}}$ 
remains positive semidefinite. 
Note that in the implementation, $D^p$ is never constructed; 
only $-(D^p)^{-1}$ is directly computed:
\[
  -(D^p)^{-1}
  = \begin{pmatrix}
      -I & O & O & O \\
      O & 0 & -\gamma(z^p)^2 & 0 \\
      O & -\gamma(z^p)^2 & 0 & 0 \\
      O & 0 & 0 & 1
    \end{pmatrix}
\]

For the \texttt{NT} direction, the expression for $M^p$ takes a different form.
From Section~\ref{sec:direction}, we have
\[
  M^p
  = \frac{1}{(\omega^p)^2} \Big(
      A^p (A^p)^T + 2 u^p (u^p)^T - 2k^p (k^p)^T
  \Big),
\]
where $u^p := A^p t^p$ and $k^p := A^p e^p$.
Similar to the HKM case, when $A^p_{\mathrm{dense}}$ is non-empty, 
the vector $u^p$ becomes dense, making $M^p$ a dense matrix.

SDPT3 applies a similar decomposition strategy by separating 
the contributions from sparse and dense columns:
\[
  M^p_{\mathrm{sparse}}
    := \frac{1}{(\omega^p)^2} \, A^p_{\mathrm{sparse}}\,(A^p_{\mathrm{sparse}})^T,
\]
\[
  U^p
    := \Bigl(
      \frac{1}{\omega^p}\,A^p_{\mathrm{dense}},
       \;\; \sqrt{2}\,u^p,
       \;\; -\sqrt{2}\,k^p
    \Bigr),
\]
\[
  D^p
    := \begin{pmatrix}
         I & O & O \\
         O & 1/(\omega^p)^2 & 0  \\
         O & 0 & -1 
       \end{pmatrix}.
\]
This decomposition satisfies $M^p = M^p_{\mathrm{sparse}} + U^p D^p (U^p)^T$,
where again the inclusion of the $k^p$ term ensures that $M^p_{\mathrm{sparse}}$ 
remains positive semidefinite.
As with the HKM direction, $D^p$ is never constructed in the implementation;
only $-(D^p)^{-1}$ is directly computed:
\[
  -(D^p)^{-1}
  = \begin{pmatrix}
      -I & O & O  \\
      O & -(\omega^p)^2 & 0  \\
      O & 0 & 1
    \end{pmatrix}.
\]


\paragraph{Linear cone:}
Consider $\mathbb{K}^p = \mathbb{R}^{n_p}_+$. 
Recall from Section~\ref{sec:direction} that
\[
  M^p = A^p \operatorname{diag}(x^p) \operatorname{diag}(z^p)^{-1} (A^p)^T.
\]
Unlike the second-order cone case, SDPT3 uses the same formula ($G^p=I$) 
for both \texttt{HKM} and \texttt{NT} directions.
When $A^p_{\mathrm{dense}}$ is non-empty, this matrix becomes dense.

To exploit sparsity, SDPT3 decomposes $M^p$ as follows.
Partition $x^p$ and $z^p$ according to the sparse/dense partition of $A^p$:
$x^p_{\mathrm{sparse}}$ and $z^p_{\mathrm{sparse}}$ contain the elements corresponding to sparse columns of $A^p$,
while $x^p_{\mathrm{dense}}$ and $z^p_{\mathrm{dense}}$ contain the elements corresponding to dense columns of $A^p$.
Then it defines:
\[
   M^p_{\mathrm{sparse}}
   = A^p_{\mathrm{sparse}}
     \,\operatorname{diag}(x^p_{\mathrm{sparse}})
     \,\operatorname{diag}(z^p_{\mathrm{sparse}})^{-1}
     \,(A^p_{\mathrm{sparse}})^T,
\]
\[
   U^p
   = A^p_{\mathrm{dense}}
     \,\operatorname{diag}(x^p_{\mathrm{dense}})^{\tfrac12}
     \,\operatorname{diag}(z^p_{\mathrm{dense}})^{-\tfrac12},
\]
\[ D^p = I. \]
This decomposition satisfies $M^p = M^p_{\mathrm{sparse}} + U^p D^p (U^p)^T$,
where $M^p_{\mathrm{sparse}}$ is positive definite.

In this simple case, $-(D^p)^{-1} = -I$.
