\section{Other computation techniques}
\subsection{Perturbation of \boldmath $M$ }
In practice, $M$ may become numerically ill-conditioned due to several factors: roundoff errors, rank-deficient constraint matrices, and the inherent nature of interior-point methods where approaching optimality ($\mu \to 0$) leads to increasingly poor conditioning as complementarity gaps narrow.
To ensure numerical stability of the Cholesky factorization, SDPT3 employs an adaptive perturbation strategy:
\[
  M \;\leftarrow\; M + \rho D_M + \lambda \mathcal{A}\mathcal{A}^T
\]
where $D_M$ is a diagonal matrix formed from the diagonal elements of $M$, and parameters $\rho, \lambda$ decrease geometrically with iterations.\footnote{
  Intuitively, the diagonal term $\rho D_M$ improves conditioning while preserving solution accuracy, while $\lambda \mathcal{A}\mathcal{A}^T$ ensures numerical stability along the constraint directions.
}
When extreme ill-conditioning is detected (e.g., condition number is greater than $10^{14}$), diagonal elements below $10^{-8}$ may be increased to $1$, but only when very few such elements exist.

These perturbations involve a delicate trade-off: while larger perturbations ensure numerical stability, they also distance the solution from the original problem, potentially compromising accuracy. SDPT3 implements a sophisticated perturbation strategy that carefully balances these competing concerns through iteration-dependent parameters and conditional application; for the complete algorithm, see the \texttt{linsysolve.m} function.


\subsection{Handling free variables}
If $\nu^p = 0$ for all $p \in P$, the problem (P)(D) can be transformed into the following 3-parameter Homogeneous self-dual (HSD) model \cite{Wright1997}.
Given an initial point 
\[
(x_0, y_0, z_0, \tau_0, \kappa_0, \theta_0) \in 
  \operatorname{int}(\mathbb{K}) \times \mathbb{R}^m \times \operatorname{int}(\mathbb{K})
  \times \mathbb{R}^1_+ \times \mathbb{R}^1_+ \times \mathbb{R}^1_+,
\]
the HSD model is formulated as:
\[
  \begin{array}{cl}
   \min_{x,y,z,\tau,\kappa,\theta} & \bar{\alpha}\,\theta \\[3pt]
   \text{s.t.}
   & \begin{pmatrix}
       0 & -\mathcal{A} & b & -\bar{b}\\
       \mathcal{A}^T & 0 & -c & \bar{c}\\
       -b^T & c^T & 0 & -\bar{g}\\
       \bar{b}^T & -\bar{c}^T & \bar{g} & 0
     \end{pmatrix}
     \begin{pmatrix} y \\ x \\ \tau \\ \theta \end{pmatrix}
   \;+\;
     \begin{pmatrix} 0 \\ z \\ \kappa \\ 0 \end{pmatrix}
   =
     \begin{pmatrix} 0 \\ 0 \\ 0 \\ \bar{\alpha} \end{pmatrix}
  \end{array}
\]
where the parameters are defined as:
\begin{align*}
    \bar{b} &= \frac{1}{\theta_0}(b\tau_0 - \mathcal{A}x_0) \\
    \bar{c} &= \frac{1}{\theta_0}(c\tau_0 - \mathcal{A}^T y_0 - z_0) \\
    \bar{g} &= \frac{1}{\theta_0}(\inprod{c}{x_0} - b^Ty_0 + \kappa_0) \\
    \bar{\alpha} &= \frac{1}{\theta_0} (\inprod{x_0}{z_0} + \tau_0 \kappa_0)
\end{align*}

According to Toh et al.~\cite{toh1999}, if the feasible region of (P)(D) is non-empty but lacks an interior point, converting to the HSD model yields superior accuracy despite increased computational cost.
This is particularly important when unrestricted blocks exist (i.e., $\exists p\in P$ such that $\mathbb{K}^p=\mathbb{R}^{n_p}$), as the dual cone $(\mathbb{K}^p)^*=\{0\}$ implies that there is no interior point in the dual feasible region.

SDPT3 implements this HSD transformation when appropriate.
The standard algorithm cannot be directly applied to the HSD model and requires modifications, which are detailed in \cite{toh1999}.

\medskip

When $\nu^p \neq 0$ for some $p \in P$, the problem (P)(D) cannot be converted to the HSD model.
As seen from \eqref{eq:Schur_complement_Mat}, unrestricted blocks (i.e., $\mathbb{K}^p = \mathbb{R}^{n_p}$) require solving augumented systems involving $A^u$, which increases computational cost.
To avoid this overhead, SDPT3 eliminates free variables $x^p \in \mathbb{R}^{n_p}$ by splitting them into non-negative components:
\[
   x^p = x^p_+ - x^p_-, \quad \text{where } (x^p_+, x^p_-) \in \mathbb{R}^{2n_p}_+
\]

This transformation, while computationally efficient, introduces numerical challenges.
As iterations progress, both $x^p_+$ and $x^p_-$ typically grow large while their dual counterparts $z^p_+$ and $z^p_-$ become small, causing the complementarity products $\operatorname{diag}(x^p_\pm) \operatorname{diag}(z^p_\pm)$ to become severely ill-conditioned.

To mitigate these issues, SDPT3 employs heuristic stabilization techniques.
At each iteration, the primal variables are recentered:
\[
   x^p_+ \leftarrow x^p_+ - 0.8 \min(x^p_+, x^p_-), \quad
   x^p_- \leftarrow x^p_- - 0.8 \min(x^p_+, x^p_-)
\]
while the dual variables are perturbed by adding $\alpha \mu e^p$, where $e^p$ is the identity element defined in Section 3.1 and $\alpha = 0.1$ is an adaptively defined small constant, to prevent them from approaching zero too rapidly.

\medskip

Similar numerical issues arise even in linear blocks ($\mathbb{K}^p=\mathbb{R}^{n_p}_+$) when pairs of variables effectively represent a single free variable.
Specifically, if variables $x^p_i$ and $x^p_j$ satisfy $(a^p_k)_i = -(a^p_k)_j$ for all constraints $k=1,\ldots,m$ and $(c^p)_i = -(c^p)_j$ with $\nu^p = 0$, they exhibit the same instability pattern.
SDPT3 detects such implicit free variables and applies the same stabilization techniques described above.


\subsection{Preprocessing for model transformation}
\subsubsection{Transforming complex semidefinite variables to real}
In applications such as control engineering, optimization problems on complex semidefinite cones may arise, but they can be reduced to problems on real semidefinite cones.

Define the following:
\begin{itemize}
    \item Complex matrix space $\mathbb{C}^{m\times n}$
    \item Set of Hermitian matrices $\mathbb{H}^n=\{a \in \mathbb{C}^{n\times n} \mid a = a^H\}$ where $a^H$ denotes the conjugate transpose
    \item Complex positive semidefinite cone $\mathbb{H}^n_+=\{a \in \mathbb{H}^n \mid z^H a z \geq 0 ~(\forall z\in \mathbb{C}^n)\}$
    \item $\bar{\mathbb{S}}^{2n}_+ = \left\{\left(\begin{smallmatrix}
      R & -S\\
      S & R
  \end{smallmatrix} \right) \in \mathbb{S}^{2n}_+ \;\middle|\; R\in \mathbb{S}^n, ~ S^T=-S\right\}.$
\end{itemize}
Any Hermitian matrix $a \in \mathbb{H}^n$ can be uniquely decomposed as $a = R + iS$ where $R = \operatorname{real}(a) \in \mathbb{S}^n$ is the real part which is a real symmetric matrix and $S = \operatorname{imag}(a) \in \mathbb{R}^{n\times n}$ is the imaginary part which is a real skew-symmetric matrix, i.e., $S^T = -S$.

We define the real embedding $\Gamma: \mathbb{H}^n\to \mathbb{S}^{2n}$ as
\[
  \Gamma(a) 
  = \begin{pmatrix}
       \operatorname{real}(a) & -\operatorname{imag}(a) \\
       \operatorname{imag}(a) & \operatorname{real}(a)
     \end{pmatrix}.
\]
Then the mapping $\Gamma$ establishes a cone isomorphism between $\mathbb{H}^n_+$ and $\bar{\mathbb{S}}^{2n}_+$:
\[
  a\in \mathbb{H}^n_+
   \;\;\Longleftrightarrow\;\;
  \Gamma(a)\in \bar{\mathbb{S}}^{2n}_+.
\]
This equivalence follows from the identity: for any $u,v\in\mathbb{R}^n$ and $z=u+iv\in\mathbb{C}^n$,
\[
  z^H a z = 
  \begin{pmatrix}u\\ v\end{pmatrix}^T \Gamma(a) \begin{pmatrix}u\\ v\end{pmatrix}.
\]
The left-hand side is non-negative for all $z \in \mathbb{C}^n$ if and only if the right-hand side is non-negative for all $(u,v) \in \mathbb{R}^{2n}$.

Thus, by transforming complex variables $x \in \mathbb{H}^{n_p}_+$ to real variables $\bar{x} = \Gamma(x) \in \bar{\mathbb{S}}^{2n_p}_+$, problems on complex semidefinite cones can be reduced to problems on real semidefinite cones.

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            2 & 1-i \\
            1+i & 3
        \end{pmatrix}}{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 
            0 & 1
        \end{pmatrix}}{x} = 4 \\
        & x\in \mathbb{H}^2_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            2 & 1 & 0 & 1 \\
            1 & 3 & -1 & 0 \\
            0 & -1 & 2 & 1 \\
            1 & 0 & 1 & 3
        \end{pmatrix}}{\bar{x}} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 & 0 \\ 
            0 & 1 & 0 & 0 \\ 
            0 & 0 & 1 & 0 \\ 
            0 & 0 & 0 & 1
        \end{pmatrix}}{\bar{x}} = 4 \\
        & \bar{x}\in \bar{\mathbb{S}}^4_+ = \left\{\begin{pmatrix}
            R & -S\\
            S & R
        \end{pmatrix} \in \mathbb{S}^4_+ \;\middle|\; R\in \mathbb{S}^2, ~ S^T=-S\right\}
    \end{array}
    \right.
\end{equation*}
\end{example}



\subsubsection{Converting diagonal blocks to linear variables}
The condition $x\in \mathbb{S}^1_+$ is equivalent to $x\in \mathbb{R}^1_+$, since a $1\times 1$ positive semidefinite matrix is simply a non-negative scalar.
In this case, treating $x$ as a non-negative real variable improves the computational efficiency of the interior-point method.
Furthermore, if the variable matrix in $\mathbb{S}^{n_p}_+$ contains isolated diagonal elements (i.e., elements that do not interact with off-diagonal entries in the constraints), converting them to non-negative real variables yields similar computational efficiency improvements.

Specifically, for an integer $i$, if the $i$-th diagonal element is isolated, i.e.,
$(c^p)_{ij}=(c^p)_{ji}=0$ and $(a^p_k)_{ij}=(a^p_k)_{ji}=0$ for all $j\neq i$ and all $k$,
SDPT3 transforms $x\in \mathbb{S}^{n_p}_+$ to $(\bar{x},\,\hat{x})\in \mathbb{S}^{n_p-1}_+\times \mathbb{R}^1_+$,
where $\bar{x}$ is the $(n_p-1)\times(n_p-1)$ matrix obtained by removing the $i$-th row and column from $x$,
and $\hat{x} = x_{ii} \geq 0$ represents the isolated diagonal element.

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 0 & 1 \\
            0 & 5 & 0 \\
            1 & 0 & 2
        \end{pmatrix}}{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 \\ 
            0 & 2 & 0 \\ 
            0 & 0 & 3
        \end{pmatrix}}{x} = 1 \\
        & x\in \mathbb{S}^3_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 1 \\
            1 & 2
        \end{pmatrix}}{\bar{x}} + 5\hat{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 0 & 3
        \end{pmatrix}}{\bar{x}} + 2\hat{x} = 1 \\
        & \bar{x}\in \mathbb{S}^2_+, \quad \hat{x} \in \mathbb{R}^1_+
    \end{array}
    \right.
\end{equation*}
\end{example}
Indeed, for example, the instance \texttt{qpG11} from SDPLIB contains many such diagonal blocks that benefit from this transformation technique.


\subsubsection{Ensuring algorithmic assumptions through variable augmentation}
The interior-point method algorithm introduced in this paper requires $m \geq 1$ and $\exists p\in P ~ \text{s.t.} ~ \nu^p=0$ (mainly for \eqref{eq:NewtonKKT} and \eqref{eq:mu}).
When these prerequisites are not satisfied, SDPT3 adds one artificial non-negative variable $x^{p_{\max} + 1}$ and the corresponding constraint
\[
  -\sum_{p\in P} \inprod{e^p}{x^p}_p + x^{p_{\max} + 1} = 0
\]
to ensure the algorithmic assumptions hold.
Specifically, this augmentation introduces the following additional problem parameters:
\begin{itemize}
    \item $\mathbb{K}^{p_{\max} + 1} = \mathbb{R}^1_+$
    \item $a^{p}_{m+1}=e^p \;\; (p \in P) \;\;$ and $\;\; a^{p_{\max} + 1}_{m+1} = 1$
    \item $b_{m+1} = 0$
    \item $c^{p_{\max} +1}=0$
    \item $\nu^{p_{\max} + 1} = 0$
\end{itemize}
Note that when $m = 0$ (no existing constraints), this creates the first constraint with $b_1 = 0$. When $m \geq 1$, this extends the existing constraint vector $b$ by appending a new element $b_{m+1} = 0$.


\subsubsection{Reordering matrices for Cholesky efficiency}
To verify the positive definiteness of variables $x^p, z^p$, SDPT3 uses Cholesky decomposition, which provides a numerically stable test.
Since the interior-point method performs Cholesky decomposition at each iteration, minimizing fill-in is crucial for computational efficiency.
SDPT3 reorders matrix variables to minimize fill-in during Cholesky decomposition.
To identify the sparsity pattern, SDPT3 first constructs the aggregate matrix
\[
  t^p = |c^p| + \sum_{k=1}^m |a^p_k|
\]
which represents the combined sparsity structure of all coefficient matrices.
The Reverse Cuthill-McKee algorithm is then applied to obtain the permutation $\sigma$ that reduces the bandwidth (the maximum distance of non-zero elements from the diagonal).
Since reducing bandwidth typically reduces fill-in during Cholesky decomposition, this reordering improves computational efficiency.\footnote{
  While the benefits are problem-dependent, this reordering often improves efficiency in practice. 
  The Newton system matrix $M = \mathcal{A}\mathcal{H}\mathcal{A}^T$ tends to inherit sparsity from the coefficient matrices $\mathcal{A}$, and iterates $x^p, z^p$ often reflect the coefficient structure $(a_k^p)$, particularly near optimality. 
  The reduced bandwidth typically leads to less fill-in during Cholesky decompositions throughout the algorithm.}
The variables are reordered according to $(\bar{x}^p)_{ij} = (x^p)_{\sigma(i)\sigma(j)}$.

\begin{example}
\[
    t^p = \begin{pmatrix}
        3 & 0 & 1 \\\
        0 & 5 & 0 \\\
        1 & 0 & 2
    \end{pmatrix}
\]
Applying the Reverse Cuthill-McKee algorithm yields $\sigma(1)=3, ~ \sigma(2)=1, ~ \sigma(3)=2$.
Transforming variables using this permutation results in $\bar{t}^p$:
\[
    \bar{t}^p = \begin{pmatrix}
        3 & 1 & 0 \\
        1 & 2 & 0 \\
        0 & 0 & 5
    \end{pmatrix}
\]
which reduces the bandwidth from 2 to 1.
\end{example}


\subsubsection{Utilizing matrix symmetry}
In blocks where $\mathbb{K}^p=\mathbb{S}_+^p$, leveraging the symmetry of coefficient matrices can reduce memory usage and computation time for matrix inner products.

Given a real symmetric matrix $A\in \mathbb{S}^n$, define $\operatorname{svec}: \mathbb{S}^n \rightarrow \mathbb{R}^{n(n+1)/2}$ for $i\leq j$ as follows:
\[ \operatorname{svec}(A)= (a_1^T, a_2^T, \ldots a_n^T)^T \]
where
\[ a_j = (f_{j,j} A_{j,j}, ~ f_{j+1,j}A_{j+1,j}, ~ \ldots ~ , ~ f_{n,j} A_{n,j})^T \in \mathbb{R}^{n-j+1} \]
\[ f_{ij}= \begin{cases} 1 & \text{if} ~ i=j \\ \sqrt{2} & \text{otherwise} \end{cases}\]

\begin{example}
\[A = \begin{pmatrix}
    1 & 2 & 3\\
    2 & 4 & 5\\
    3 & 5 & 6
\end{pmatrix} \rightarrow \operatorname{svec}(A) = (1, ~ 2\sqrt{2}, ~ 3\sqrt{2}, ~ 4, ~ 5\sqrt{2}, ~ 6)^T\]
\end{example}

The following equality holds:
\[\inprod{A}{A} = \operatorname{svec}(A)^T \operatorname{svec}(A).\]
Thus, by storing $\operatorname{svec}(A)$ instead of $A$, memory usage and computation time for inner products can be approximately halved.
SDPT3 adopts the $\operatorname{svec}$ format for storing coefficient matrices $a^p_k$ (while $c^p$, $x^p$, and $z^p$ are stored in matrix form).


