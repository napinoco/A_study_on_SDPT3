\section{Other computation techniques}
\subsection{Perturbation of \boldmath $M$ }
In practice, $M$ may become numerically ill-conditioned due to several factors: roundoff errors, rank-deficient constraint matrices, and the inherent nature of interior-point methods where approaching optimality ($\mu \to 0$) leads to increasingly poor conditioning as complementarity gaps narrow.
To ensure numerical stability of the Cholesky factorization, SDPT3 employs an adaptive perturbation strategy:
\[
  M \;\leftarrow\; M + \rho D_M + \lambda \mathcal{A}\mathcal{A}^T
\]
where $D_M$ is a diagonal matrix formed from the diagonal elements of $M$, and parameters $\rho, \lambda$ decrease geometrically with iterations.\footnote{
  Intuitively, the diagonal term $\rho D_M$ improves conditioning while preserving solution accuracy, while $\lambda \mathcal{A}\mathcal{A}^T$ ensures numerical stability along the constraint directions.
}
When extreme ill-conditioning is detected (e.g., condition number is greater than $10^{14}$), diagonal elements below $10^{-8}$ may be increased to $1$, but only when very few such elements exist.

These perturbations involve a delicate trade-off: while larger perturbations ensure numerical stability, they also distance the solution from the original problem, potentially compromising accuracy. SDPT3 implements a sophisticated perturbation strategy that carefully balances these competing concerns through iteration-dependent parameters and conditional application; for the complete algorithm, see the \texttt{linsysolve.m} function.


\subsection{Handling free variables}
If $\nu^p = 0$ for all $p \in P$, the problem (P)(D) can be transformed into the following 3-parameter Homogeneous self-dual (HSD) model \cite{Wright1997}.
Given an initial point 
\[
(x_0, y_0, z_0, \tau_0, \kappa_0, \theta_0) \in 
  \operatorname{int}(\mathbb{K}) \times \mathbb{R}^m \times \operatorname{int}(\mathbb{K})
  \times \mathbb{R}^1_+ \times \mathbb{R}^1_+ \times \mathbb{R}^1_+,
\]
the HSD model is formulated as:
\[
  \begin{array}{cl}
   \min_{x,y,z,\tau,\kappa,\theta} & \bar{\alpha}\,\theta \\[3pt]
   \text{s.t.}
   & \begin{pmatrix}
       0 & -\mathcal{A} & b & -\bar{b}\\
       \mathcal{A}^T & 0 & -c & \bar{c}\\
       -b^T & c^T & 0 & -\bar{g}\\
       \bar{b}^T & -\bar{c}^T & \bar{g} & 0
     \end{pmatrix}
     \begin{pmatrix} y \\ x \\ \tau \\ \theta \end{pmatrix}
   \;+\;
     \begin{pmatrix} 0 \\ z \\ \kappa \\ 0 \end{pmatrix}
   =
     \begin{pmatrix} 0 \\ 0 \\ 0 \\ \bar{\alpha} \end{pmatrix}
  \end{array}
\]
where the parameters are defined as:
\begin{align*}
    \bar{b} &= \frac{1}{\theta_0}(b\tau_0 - \mathcal{A}x_0) \\
    \bar{c} &= \frac{1}{\theta_0}(c\tau_0 - \mathcal{A}^T y_0 - z_0) \\
    \bar{g} &= \frac{1}{\theta_0}(\inprod{c}{x_0} - b^Ty_0 + \kappa_0) \\
    \bar{\alpha} &= \frac{1}{\theta_0} (\inprod{x_0}{z_0} + \tau_0 \kappa_0)
\end{align*}

According to Toh et al.~\cite{toh1999}, if the feasible region of (P)(D) is non-empty but lacks an interior point, converting to the HSD model yields superior accuracy despite increased computational cost.
This is particularly important when unrestricted blocks exist (i.e., $\exists p\in P$ such that $\mathbb{K}^p=\mathbb{R}^{n_p}$), as the dual cone $(\mathbb{K}^p)^*=\{0\}$ implies that there is no interior point in the dual feasible region.

SDPT3 implements this HSD transformation when appropriate.
The standard algorithm cannot be directly applied to the HSD model and requires modifications, which are detailed in \cite{toh1999}.

\medskip

When $\nu^p \neq 0$ for some $p \in P$, the problem (P)(D) cannot be converted to the HSD model.
As seen from \eqref{eq:Schur_complement_Mat}, unrestricted blocks (i.e., $\mathbb{K}^p = \mathbb{R}^{n_p}$) require solving augumented systems involving $A^u$, which increases computational cost.
To avoid this overhead, SDPT3 eliminates free variables $x^p \in \mathbb{R}^{n_p}$ by splitting them into non-negative components:
\[
   x^p = x^p_+ - x^p_-, \quad \text{where } (x^p_+, x^p_-) \in \mathbb{R}^{2n_p}_+
\]

This transformation, while computationally efficient, introduces numerical challenges.
As iterations progress, both $x^p_+$ and $x^p_-$ typically grow large while their dual counterparts $z^p_+$ and $z^p_-$ become small, causing the complementarity products $\operatorname{diag}(x^p_\pm) \operatorname{diag}(z^p_\pm)$ to become severely ill-conditioned.

To mitigate these issues, SDPT3 employs heuristic stabilization techniques.
At each iteration, the primal variables are recentered:
\[
   x^p_+ \leftarrow x^p_+ - 0.8 \min(x^p_+, x^p_-), \quad
   x^p_- \leftarrow x^p_- - 0.8 \min(x^p_+, x^p_-)
\]
while the dual variables are perturbed by adding $\alpha \mu e^p$, where $e^p$ is the identity element defined in Section 3.1 and $\alpha = 0.1$ is an adaptively defined small constant, to prevent them from approaching zero too rapidly.

\medskip

Similar numerical issues arise even in linear blocks ($\mathbb{K}^p=\mathbb{R}^{n_p}_+$) when pairs of variables effectively represent a single free variable.
Specifically, if variables $x^p_i$ and $x^p_j$ satisfy $(a^p_k)_i = -(a^p_k)_j$ for all constraints $k=1,\ldots,m$ and $(c^p)_i = -(c^p)_j$ with $\nu^p = 0$, they exhibit the same instability pattern.
SDPT3 detects such implicit free variables and applies the same stabilization techniques described above.


\subsection{Preprocessing for model transformation}
\subsubsection{Complex input}
In applications such as control engineering, optimization problems on complex semidefinite cones may arise, but they can be reduced to problems on real semidefinite cones.
Define the following:
\begin{itemize}
    \item Complex matrix space $\mathbb{C}^{m\times n}$
    \item Set of Hermitian matrices $\mathbb{H}^n=\{a \in \mathbb{C}^{n\times n} \mid a = a^H\}$ where $a^H$ denotes the conjugate transpose of $a$.
    \item Complex positive semidefinite symmetric matrix cone $\mathbb{H}^n_+=\{a \in \mathbb{H}^n \mid x^T a x \geq 0 ~(\forall x\in \mathbb{C}^n)\}$
    \item $\bar{\mathbb{S}}^{2n}_+ = \{(\begin{smallmatrix}
    A & B\\
    C & D
\end{smallmatrix}) \in \mathbb{S}^{2n}_+ \mid A=D\in \mathbb{S}^n, ~ B=-B^T=C=-C^T\}$
\end{itemize}
It is known that $\mathbb{H}^n_+$ and $\bar{\mathbb{S}}^{2n}_+$ are algebraically isomorphic.
Define the mapping $\Gamma: \mathbb{H}^n\to \mathbb{S}^{2n}$ as
\[
  \Gamma(x)
  := \begin{pmatrix}
       \operatorname{real}(x) & -\operatorname{imag}(x) \\
       \operatorname{imag}(x) & \operatorname{real}(x)
     \end{pmatrix}
\]
For example,
\[
  x\in \mathbb{H}^n_+
   \;\;\Longleftrightarrow\;\;
  \Gamma(x)\in \mathbb{S}^{2n}_+
\]
Thus, by transforming variables using $\Gamma(x)= y \in \bar{\mathbb{S}}^{2n_p}_+$, problems on complex semidefinite cones can be reduced to problems on real semidefinite cones.

\subsubsection{Detect diagonal block of $\mathbb{K}^p=\mathbb{S}^{n_p}_+$}

The condition $x\in \mathbb{S}^1_+$ is equivalent to $x\in \mathbb{R}^1_+$.
In this case, treating $x$ as a simple non-negative real variable improves the computational efficiency of the interior-point method.
Furthermore, if there are diagonal blocks within the variable matrix of $\mathbb{S}^{n_p}_+$, converting them to non-negative real variables yields similar computational efficiency improvements.

Specifically, for an integer $i$,
if $(c^p)_{ij}=(c^p)_{ji}=0$ and $(a^p_k)_{ij}=(a^p_k)_{ji}=0$ 
($\forall j\neq i,\,\forall k$),
transform $x\in \mathbb{S}^{n_p}_+$ to $(\bar{x},\,\hat{x})\in \mathbb{S}^{n_p-1}_+\times \mathbb{R}^1_+$.
Here, $\bar{x}$ is the matrix obtained by removing the $i$-th row and column from $x$, and $x_{ij}= x_{ji}=0$ ($\forall j\neq i$).

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 0 & 1 \\
            0 & 5 & 0 \\
            1 & 0 & 2
        \end{pmatrix}}{X} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 \\ 
            0 & 2 & 0 \\ 
            0 & 0 & 3
        \end{pmatrix}}{X} = 1 \\
        & X\in \mathbb{S}^3_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 1 \\
            1 & 2
        \end{pmatrix}}{\bar{X}} + 5\hat{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 0 & 3
        \end{pmatrix}}{\bar{X}} + 2\hat{x} = 1 \\
        & \bar{X}\in \mathbb{S}^2_+, \quad \hat{x} \in \mathbb{R}^1_+
    \end{array}
    \right.
\end{equation*}
\end{example}
Indeed, in SDPLIB's xxx, there are many such diagonal blocks.

\subsubsection{Artificial linear block}

The interior-point method algorithm introduced in this paper assumes $m \geq 1$ and $\exists p\in P ~ \text{s.t.} ~ \nu^p=0$ (mainly for \eqref{eq:NewtonKKT} and \eqref{eq:mu}).

If these conditions are not met, adding one redundant non-negative variable $x^{p_{\max} + 1}$ and one constraint
\[
  -\sum_{p\in P} \inprod{e^p}{x^p}_p + x^{p_{\max} + 1} = 0
\]
can satisfy the assumptions.
Specifically, set new parameters as follows:
\begin{itemize}
    \item $\mathbb{K}^{p_{\max} + 1} = \mathbb{R}^1_+$
    \item $a^{p}_1=e^p ~ (p \in P)$, $a^{p_{\max} + 1}_1 = 1$
    \item $b = 0$
    \item $c^{p_{\max} +1}=0$
    \item $\nu^{p_{\max} + 1} = 0$
\end{itemize}

\subsubsection{Reordering Matrix Variables}
To determine the positive definiteness of variables $x^p, z^p$, checking whether Cholesky decomposition succeeds is numerically stable.
Since the interior-point method performs this Cholesky decomposition at each iteration, minimizing fill-in is important.
SDPT3 reorders matrix variables to minimize fill-in during Cholesky decomposition.
Specifically, define
\[
  t^p = |c^p| + \sum_{k=1}^m |a^p_k|
\]
and apply the Reverse Cuthill--Mckee Algorithm to obtain the permutation $\sigma$.
Finally, transform variables as $(\bar{x}^p)_{ij} = (x^p)_{\sigma(i)\sigma(j)}$.

\begin{example}
\[
    t^p = \begin{pmatrix}
        3 & 0 & 1 \\\
        0 & 5 & 0 \\\
        1 & 0 & 2
    \end{pmatrix}
\] Applying the Reverse Cuthill-Mckee Algorithm yields $\sigma(1)=3, ~ \sigma(2)=1, ~ \sigma(3)=2$.
Transforming variables using this permutation results in $\bar{t}^p$:
\[
    \bar{t}^p = \begin{pmatrix}
        3 & 1 & 0 \\
        1 & 2 & 0 \\
        0 & 0 & 5
    \end{pmatrix}
\]
which indeed has a smaller bandwidth.
\end{example}

\subsubsection{Utilizing matrix symmetry}
In blocks where $\mathbb{K}^p=\mathbb{S}_+^p$, leveraging the symmetry of coefficient matrices can reduce memory usage and computation time for matrix inner products.

Given a real symmetric matrix $A\in \mathbb{S}^n$, define $\operatorname{svec}: \mathbb{S}^n \rightarrow \mathbb{R}^{n(n+1)/2}$ for $i\leq j$ as follows:
\[ \operatorname{svec}(A)= (a_1^T, a_2^T, \ldots a_n^T)^T \]
where
\[ a_j = (f_{j,j} A_{j,j}, ~ f_{j+1,j}A_{j+1,j}, ~ \ldots ~ , ~ f_{n,j} A_{n,j})^T \in \mathbb{R}^{n-j+1} \]
\[ f_{ij}= \begin{cases} 1 & \text{if} ~ i=j \\ \sqrt{2} & \text{otherwise} \end{cases}\]

\begin{example}
\[A = \begin{pmatrix}
    1 & 2 & 3\\
    2 & 4 & 5\\
    3 & 5 & 6
\end{pmatrix} \rightarrow \operatorname{svec}(A) = (1, ~ 2\sqrt{2}, ~ 3\sqrt{2}, ~ 4, ~ 5\sqrt{2}, ~ 6)^T\]
\end{example}

The following equality holds:
\[\inprod{A}{A} = \operatorname{svec}(A)^T \operatorname{svec}(A).\]
Thus, by storing $\operatorname{svec}(A)$ instead of $A$, memory usage and computation time for inner products can be approximately halved.
SDPT3 adopts the $\operatorname{svec}$ format for storing coefficient matrices $a^p_k$ (while $c^p$, $x^p$, and $z^p$ are stored in matrix form).


