\section{Other computation techniques}\label{sec:other_computation}
\subsection{Perturbation of \boldmath $M$ }
In practice, $M_{\text{sparse}}$ may become numerically ill-conditioned due to several factors: 
roundoff errors, rank-deficient constraint matrices, and the inherent nature of interior-point methods where approaching optimality ($\mu \to 0$) leads to increasingly poor conditioning as complementarity gaps narrow.
To ensure numerical stability of the Cholesky factorization, SDPT3 employs an adaptive perturbation strategy:
\[
  M_{\text{sparse}} \;\leftarrow\; M_{\text{sparse}} + \rho D_{M_{\text{sparse}}} + \lambda \sum_{p\in P\setminus P^{\text{u}}}A^p_{\text{sparse}} (A^p_{\text{sparse}})^T
\]
where $D_{M_{\text{sparse}}}$ is a diagonal matrix formed from the diagonal elements of $M_{\text{sparse}}$, and parameters $\rho, \lambda$ decrease geometrically with iterations.\footnote{
  Intuitively, the diagonal term $\rho D_{M_{\text{sparse}}}$ improves conditioning while preserving solution accuracy, while $\lambda \mathcal{A}\mathcal{A}^T$ ensures numerical stability along the constraint directions.
}
When extreme ill-conditioning is detected (e.g., condition number is greater than $10^{14}$), diagonal elements below $10^{-8}$ may be increased to $1$, but only when very few such elements exist.

These perturbations involve a delicate trade-off: while larger perturbations ensure numerical stability, they also distance the solution from the original problem, potentially compromising accuracy. SDPT3 implements a sophisticated perturbation strategy that carefully balances these competing concerns through iteration-dependent parameters and conditional application; for the complete algorithm, see the \texttt{linsysolve.m} function.


\subsection{Handling unrestricted variables}
If $\nu^p = 0$ for all $p \in P$, the problem (P) and (D) can be transformed into the following 3-parameter Homogeneous self-dual (HSD) model \cite{Wright1997}.
Given an initial point 
\[
(x_0, y_0, z_0, \tau_0, \kappa_0, \theta_0) \in 
  \operatorname{int}(\mathbb{K}) \times \mathbb{R}^m \times \operatorname{int}(\mathbb{K})
  \times \mathbb{R}^1_+ \times \mathbb{R}^1_+ \times \mathbb{R}^1_+,
\]
the HSD model is formulated as:
\[
  \begin{array}{cl}
   \min_{x,y,z,\tau,\kappa,\theta} & \bar{\alpha}\,\theta \\[3pt]
   \text{s.t.}
   & \begin{pmatrix}
       0 & -\mathcal{A} & b & -\bar{b}\\
       \mathcal{A}^T & 0 & -c & \bar{c}\\
       -b^T & c^T & 0 & -\bar{g}\\
       \bar{b}^T & -\bar{c}^T & \bar{g} & 0
     \end{pmatrix}
     \begin{pmatrix} y \\ x \\ \tau \\ \theta \end{pmatrix}
   \;+\;
     \begin{pmatrix} 0 \\ z \\ \kappa \\ 0 \end{pmatrix}
   =
     \begin{pmatrix} 0 \\ 0 \\ 0 \\ \bar{\alpha} \end{pmatrix}
  \end{array}
\]
where the parameters are defined as:
\begin{align*}
    \bar{b} &= \frac{1}{\theta_0}(b\tau_0 - \mathcal{A}x_0) \\
    \bar{c} &= \frac{1}{\theta_0}(c\tau_0 - \mathcal{A}^T y_0 - z_0) \\
    \bar{g} &= \frac{1}{\theta_0}(\inprod{c}{x_0} - b^Ty_0 + \kappa_0) \\
    \bar{\alpha} &= \frac{1}{\theta_0} (\inprod{x_0}{z_0} + \tau_0 \kappa_0)
\end{align*}

Toh et al.~\cite{toh1999} reported that if the feasible region of (P) and (D) is non-empty but lacks an interior point, converting to the HSD model yields superior accuracy despite increased computational cost.
This is particularly important when unrestricted blocks exist (i.e., $\exists p\in P$ such that $\mathbb{K}^p=\mathbb{R}^{n_p}$), as the dual cone $(\mathbb{K}^p)^*=\{0\}$ implies that there is no interior point in the dual feasible region.
SDPT3 applies this HSD transformation automatically when $\nu^p = 0$ for all $p \in P$ and $\mathbb{K}^p=\mathbb{R}^{n_p}$ for some $p\in P$.
The standard algorithm cannot be directly applied to the HSD model and requires modifications, which are detailed in \cite{toh1999}.

\medskip

When $\nu^p \neq 0$ for some $p \in P$, the problem (P) and (D) cannot be converted to the HSD model.
As seen from \eqref{eq:Schur_complement_Mat}, unrestricted blocks (i.e., $\mathbb{K}^p = \mathbb{R}^{n_p}$) require solving augumented systems involving $A^{\text{u}}$, which increases computational cost.
To avoid this overhead, SDPT3 eliminates unrestricted variables $x^p \in \mathbb{R}^{n_p}$ by splitting them into non-negative components:
\[
   x^p = x^p_+ - x^p_-, \quad \text{where } (x^p_+, x^p_-) \in \mathbb{R}^{2n_p}_+
\]

This transformation, while computationally efficient, introduces numerical challenges.
As iterations progress, both $x^p_+$ and $x^p_-$ typically grow large while their dual counterparts $z^p_+$ and $z^p_-$ become small, causing the complementarity products $\operatorname{diag}(x^p_\pm) \operatorname{diag}(z^p_\pm)$ to become severely ill-conditioned.

To mitigate these issues, SDPT3 employs heuristic stabilization techniques.
At each iteration, the primal variables are recentered:
\[
   x^p_+ \leftarrow x^p_+ - 0.8 \min(x^p_+, x^p_-), \quad
   x^p_- \leftarrow x^p_- - 0.8 \min(x^p_+, x^p_-)
\]
while the dual variables are perturbed by adding $\alpha \mu e^p$, where $e^p$ is the identity element defined in Section 3.1 and $\alpha = 0.1$ is an adaptively defined small constant, to prevent them from approaching zero too rapidly.

\medskip

Similar numerical issues arise even in linear blocks ($\mathbb{K}^p=\mathbb{R}^{n_p}_+$) when pairs of variables effectively represent a single unrestricted variable.
Specifically, if variables $x^p_i$ and $x^p_j$ satisfy $(a^p_k)_i = -(a^p_k)_j$ for all constraints $k=1,\ldots,m$ and $(c^p)_i = -(c^p)_j$ with $\nu^p = 0$, they exhibit the same instability pattern.
SDPT3 detects such implicit unrestricted variables and applies the same stabilization techniques described above.


\subsection{Preprocessing for model transformation}
\subsubsection{Transforming complex semidefinite variables to real}
In applications such as control engineering, optimization problems on complex semidefinite cones may arise, but they can be reduced to problems on real semidefinite cones.

Define the following:
\begin{itemize}
    \item Complex matrix space $\mathbb{C}^{m\times n}$
    \item Set of Hermitian matrices $\mathbb{H}^n=\{a \in \mathbb{C}^{n\times n} \mid a = a^H\}$ where $a^H$ denotes the conjugate transpose
    \item Complex positive semidefinite cone $\mathbb{H}^n_+=\{a \in \mathbb{H}^n \mid z^H a z \geq 0 ~(\forall z\in \mathbb{C}^n)\}$
    \item $\bar{\mathbb{S}}^{2n}_+ = \left\{\left(\begin{smallmatrix}
      R & -S\\
      S & R
  \end{smallmatrix} \right) \in \mathbb{S}^{2n}_+ \;\middle|\; R\in \mathbb{S}^n, ~ S^T=-S\right\}.$
\end{itemize}
Any Hermitian matrix $a \in \mathbb{H}^n$ can be uniquely decomposed as $a = R + iS$ where $R = \operatorname{real}(a) \in \mathbb{S}^n$ is the real part which is a real symmetric matrix and $S = \operatorname{imag}(a) \in \mathbb{R}^{n\times n}$ is the imaginary part which is a real skew-symmetric matrix, i.e., $S^T = -S$.

We define the real embedding $\Gamma: \mathbb{H}^n\to \mathbb{S}^{2n}$ as
\[
  \Gamma(a) 
  = \begin{pmatrix}
       \operatorname{real}(a) & -\operatorname{imag}(a) \\
       \operatorname{imag}(a) & \operatorname{real}(a)
     \end{pmatrix}.
\]
Then the mapping $\Gamma$ establishes a cone isomorphism between $\mathbb{H}^n_+$ and $\bar{\mathbb{S}}^{2n}_+$:
\[
  a\in \mathbb{H}^n_+
   \;\;\Longleftrightarrow\;\;
  \Gamma(a)\in \bar{\mathbb{S}}^{2n}_+.
\]
This equivalence follows from the identity: for any $u,v\in\mathbb{R}^n$ and $z=u+iv\in\mathbb{C}^n$,
\[
  z^H a z = 
  \begin{pmatrix}u\\ v\end{pmatrix}^T \Gamma(a) \begin{pmatrix}u\\ v\end{pmatrix}.
\]
The left-hand side is non-negative for all $z \in \mathbb{C}^n$ if and only if the right-hand side is non-negative for all $(\begin{smallmatrix}u\\v\end{smallmatrix}) \in \mathbb{R}^{2n}$.

Thus, by transforming complex variables $x \in \mathbb{H}^{n_p}_+$ to real variables $\bar{x} = \Gamma(x) \in \bar{\mathbb{S}}^{2n_p}_+$, problems on complex semidefinite cones can be reduced to problems on real semidefinite cones.

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            2 & 1-i \\
            1+i & 3
        \end{pmatrix}}{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 
            0 & 1
        \end{pmatrix}}{x} = 4 \\
        & x\in \mathbb{H}^2_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            2 & 1 & 0 & 1 \\
            1 & 3 & -1 & 0 \\
            0 & -1 & 2 & 1 \\
            1 & 0 & 1 & 3
        \end{pmatrix}}{\bar{x}} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 & 0 \\ 
            0 & 1 & 0 & 0 \\ 
            0 & 0 & 1 & 0 \\ 
            0 & 0 & 0 & 1
        \end{pmatrix}}{\bar{x}} = 4 \\
        & \bar{x}\in \bar{\mathbb{S}}^4_+ = \left\{\begin{pmatrix}
            R & -S\\
            S & R
        \end{pmatrix} \in \mathbb{S}^4_+ \;\middle|\; R\in \mathbb{S}^2, ~ S^T=-S\right\}
    \end{array}
    \right.
\end{equation*}
\end{example}



\subsubsection{Converting diagonal blocks to linear variables}
The condition $x\in \mathbb{S}^1_+$ is equivalent to $x\in \mathbb{R}^1_+$, since a $1\times 1$ positive semidefinite matrix is simply a non-negative scalar.
In this case, treating $x$ as a non-negative real variable improves the computational efficiency of the interior-point method.
Furthermore, if the variable matrix in $\mathbb{S}^{n_p}_+$ contains isolated diagonal elements (i.e., elements that do not interact with off-diagonal entries in the constraints), converting them to non-negative real variables yields similar computational efficiency improvements.

Specifically, for an integer $i$, if the $i$-th diagonal element is isolated, i.e.,
$(c^p)_{ij}=(c^p)_{ji}=0$ and $(a^p_k)_{ij}=(a^p_k)_{ji}=0$ for all $j\neq i$ and all $k$,
SDPT3 transforms $x\in \mathbb{S}^{n_p}_+$ to $(\bar{x},\,\hat{x})\in \mathbb{S}^{n_p-1}_+\times \mathbb{R}^1_+$,
where $\bar{x}$ is the $(n_p-1)\times(n_p-1)$ matrix obtained by removing the $i$-th row and column from $x$,
and $\hat{x} = x_{ii} \geq 0$ represents the isolated diagonal element.

\begin{example}
\begin{equation*}
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 0 & 1 \\
            0 & 5 & 0 \\
            1 & 0 & 2
        \end{pmatrix}}{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 & 0 \\ 
            0 & 2 & 0 \\ 
            0 & 0 & 3
        \end{pmatrix}}{x} = 1 \\
        & x\in \mathbb{S}^3_+ 
    \end{array}
    \right.
    \Longleftrightarrow
    \left|
    \begin{array}{cl}
        \min & \inprod{\begin{pmatrix}
            3 & 1 \\
            1 & 2
        \end{pmatrix}}{\bar{x}} + 5\hat{x} \\
        s.t. 
        & \inprod{\begin{pmatrix}
            1 & 0 \\ 0 & 3
        \end{pmatrix}}{\bar{x}} + 2\hat{x} = 1 \\
        & \bar{x}\in \mathbb{S}^2_+, \quad \hat{x} \in \mathbb{R}^1_+
    \end{array}
    \right.
\end{equation*}
\end{example}
Indeed, for example, the instance \texttt{qpG11} from SDPLIB contains many such diagonal blocks that benefit from this transformation technique.


\subsubsection{Ensuring algorithmic assumptions through variable augmentation}
The interior-point method algorithm introduced in this paper requires $m \geq 1$ and $\exists p\in P ~ \text{s.t.} ~ \nu^p=0$ (mainly for \eqref{eq:NewtonKKT} and \eqref{eq:mu}).
When these prerequisites are not satisfied, SDPT3 adds one artificial non-negative variable $x^{p_{\max} + 1}$ and the corresponding constraint
\[
  -\sum_{p\in P} \inprod{e^p}{x^p}_p + x^{p_{\max} + 1} = 0
\]
to ensure the algorithmic assumptions hold.
Specifically, this augmentation introduces the following additional problem parameters:
\begin{itemize}
    \item $\mathbb{K}^{p_{\max} + 1} = \mathbb{R}^1_+$
    \item $a^{p}_{m+1}=e^p \;\; (p \in P) \;\;$ and $\;\; a^{p_{\max} + 1}_{m+1} = 1$
    \item $b_{m+1} = 0$
    \item $c^{p_{\max} +1}=0$
    \item $\nu^{p_{\max} + 1} = 0$
\end{itemize}
Note that when $m = 0$ (no existing constraints), this creates the first constraint with $b_1 = 0$. When $m \geq 1$, this extends the existing constraint vector $b$ by appending a new element $b_{m+1} = 0$.


\subsubsection{Reordering matrices for Cholesky efficiency}
To verify the positive definiteness of variables $x^p, z^p$, SDPT3 uses Cholesky decomposition, which provides a numerically stable test.
Since the interior-point method performs Cholesky decomposition at each iteration, minimizing fill-in is crucial for computational efficiency.
SDPT3 reorders matrix variables to minimize fill-in during Cholesky decomposition.
To identify the sparsity pattern, SDPT3 first constructs the aggregate matrix
\[
  t^p = |c^p| + \sum_{k=1}^m |a^p_k|
\]
which represents the combined sparsity structure of all coefficient matrices.
The Reverse Cuthill-McKee algorithm \cite{Cuthill1969,Golub2013} is then applied to obtain the permutation $\sigma$ that reduces the bandwidth (the maximum distance of non-zero elements from the diagonal).
Since reducing bandwidth typically reduces fill-in during Cholesky decomposition, this reordering improves computational efficiency.\footnote{
  While the benefits are problem-dependent, this reordering often improves efficiency in practice. 
  The Newton system matrix $M = \mathcal{A}\mathcal{H}\mathcal{A}^T$ tends to inherit sparsity from the coefficient matrices $\mathcal{A}$, and iterates $x^p, z^p$ often reflect the coefficient structure $(a_k^p)$, particularly near optimality. 
  The reduced bandwidth typically leads to less fill-in during Cholesky decompositions throughout the algorithm.}
The variables are reordered according to $(\bar{x}^p)_{ij} = (x^p)_{\sigma(i)\sigma(j)}$.

\begin{example}
\[
    t^p = \begin{pmatrix}
        3 & 0 & 1 \\\
        0 & 5 & 0 \\\
        1 & 0 & 2
    \end{pmatrix}
\]
Applying the Reverse Cuthill-McKee algorithm yields $\sigma(1)=3, ~ \sigma(2)=1, ~ \sigma(3)=2$.
Transforming variables using this permutation results in $\bar{t}^p$:
\[
    \bar{t}^p = \begin{pmatrix}
        3 & 1 & 0 \\
        1 & 2 & 0 \\
        0 & 0 & 5
    \end{pmatrix}
\]
which reduces the bandwidth from 2 to 1.
\end{example}


\subsubsection{Utilizing matrix symmetry}
In blocks where $\mathbb{K}^p=\mathbb{S}_+^p$, exploiting the symmetry of matrices reduces memory usage and computation time for inner products.
Given a real symmetric matrix $a\in \mathbb{S}^n$, the symmetric vectorization operator $\operatorname{svec}: \mathbb{S}^n \rightarrow \mathbb{R}^{n(n+1)/2}$ extracts the upper triangular part column by column, scaling off-diagonal elements by $\sqrt{2}$ to preserve inner products:
\[ \operatorname{svec}(a) = (a_{11}, \quad \sqrt{2}\,a_{12},\, a_{22},\, \quad \sqrt{2}\,a_{13},\, \sqrt{2}\,a_{23},\, a_{33},\, \quad \ldots \quad  \sqrt{2}\,a_{1n},\, \sqrt{2}\,a_{2n},\, \ldots,\, a_{nn})^T \]
\begin{example}
\[a = \begin{pmatrix}
    1 & 2 & 3\\
    2 & 4 & 5\\
    3 & 5 & 6
\end{pmatrix} \rightarrow \operatorname{svec}(a) = (1, ~ 2\sqrt{2}, ~ 4, ~ 3\sqrt{2}, ~ 5\sqrt{2}, ~ 6)^T\]
\end{example}

\medskip

Indeed, the following equality holds for any symmetric matrices $a, b \in \mathbb{S}^n$:
\[\inprod{a}{b} = \operatorname{svec}(a)^T \operatorname{svec}(b).\]
Therefore, storing $\operatorname{svec}(a)$ instead of $a$ halves memory usage while making inner product computations more efficient.
SDPT3 stores coefficient matrices $a^p_k$ in the $\operatorname{svec}$ format for efficiency, while maintaining $c^p$, $x^p$, and $z^p$ in matrix form for algorithmic operations.


